version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR:-LocalExecutor}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:lakehouse@postgres:5432/airflow
    AIRFLOW__CORE__PARALLELISM: ${AIRFLOW_PARALLELISM:-32}
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: ${AIRFLOW_MAX_ACTIVE_RUNS:-16}
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: ${AIRFLOW_MAX_ACTIVE_TASKS:-16}
    AIRFLOW__SCHEDULER__MAX_THREADS: ${AIRFLOW_SCHEDULER_THREADS:-4}
    AIRFLOW__WEBSERVER__SECRET_KEY: lakehouse
    AIRFLOW__CORE__FERNET_KEY: 'YXNkZmFzZGZhc2RmYXNkZmFzZGZhc2RmYXNkZmFzZGY='
    # FIXED: Install DuckDB and dependencies for Issue #2
    _PIP_ADDITIONAL_REQUIREMENTS: 'duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0'
  volumes:
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/dags:/opt/airflow/dags
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/logs:/opt/airflow/logs
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy

networks:
  lakehouse:
    driver: bridge

services:
  # PostgreSQL Database
  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: airflow
      POSTGRES_PASSWORD: lakehouse
      POSTGRES_USER: postgres
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/postgres:/var/lib/postgresql/data
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: ${POSTGRES_MEMORY_LIMIT:-2G}
        reservations:
          memory: ${POSTGRES_MEMORY_RESERVATION:-512M}

  # MinIO Object Storage
  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/minio:/data
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 2s
      retries: 5
    deploy:
      resources:
        limits:
          memory: ${MINIO_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${MINIO_MEMORY_RESERVATION:-1G}

  # Initialization Service
  lakehouse-init:
    image: alpine:3.18
    depends_on:
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    entrypoint:
      - /bin/sh
      - /host/init-all-in-one.sh
    environment:
      INSTALL_SAMPLES: "true"
      LAKEHOUSE_ROOT: /mnt/lakehouse
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}:/mnt/lakehouse
      - ./init-all-in-one.sh:/host/init-all-in-one.sh:ro
    networks:
      - lakehouse
    restart: "no"

  # Spark Master
  spark-master:
    image: bitnami/spark:3.5.0
    depends_on:
      minio:
        condition: service_healthy
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_MEMORY: ${SPARK_MASTER_MEMORY:-2g}
      SPARK_MASTER_CORES: ${SPARK_MASTER_CORES:-2}
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/jobs:/opt/spark/jobs
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: ${SPARK_MASTER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${SPARK_MASTER_MEMORY_RESERVATION:-1G}

  # Spark Worker
  spark-worker:
    image: bitnami/spark:3.5.0
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-8g}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-4}
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/jobs:/opt/spark/jobs
    networks:
      - lakehouse
    deploy:
      resources:
        limits:
          memory: ${SPARK_WORKER_MEMORY_LIMIT:-16G}
          cpus: ${SPARK_WORKER_CPU_LIMIT:-8}
        reservations:
          memory: ${SPARK_WORKER_MEMORY_RESERVATION:-4G}
          cpus: ${SPARK_WORKER_CPU_RESERVATION:-2}

  # JupyterLab
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.5.0
    depends_on:
      spark-master:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: lakehouse
      SPARK_MASTER: spark://spark-master:7077
    ports:
      - "9040:8888"
    command: >
      bash -c "
      pip install duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0 pandas &&
      start-notebook.sh
      "
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/notebooks:/home/jovyan/notebooks
    networks:
      - lakehouse
    deploy:
      resources:
        limits:
          memory: ${JUPYTER_MEMORY_LIMIT:-8G}
        reservations:
          memory: ${JUPYTER_MEMORY_RESERVATION:-2G}

  # Airflow Database Initialization
  airflow-init:
    <<: *airflow-common
    depends_on:
      postgres:
        condition: service_healthy
      lakehouse-init:
        condition: service_completed_successfully
    command: >
      bash -c "
      pip install duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0 &&
      airflow db init &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true
      "
    restart: "no"

  # Airflow Scheduler
  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: scheduler
    deploy:
      resources:
        limits:
          memory: ${AIRFLOW_SCHEDULER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${AIRFLOW_SCHEDULER_MEMORY_RESERVATION:-1G}

  # Airflow Webserver
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-scheduler:
        condition: service_started
    command: webserver
    ports:
      - "9020:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: ${AIRFLOW_WEBSERVER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${AIRFLOW_WEBSERVER_MEMORY_RESERVATION:-1G}

  # Apache Superset - FIXED for Issue #1
  superset:
    image: apache/superset:latest
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/bash
      - -c
      - |
        # FIXED: Install latest DuckDB packages for Issue #1
        pip install psycopg2-binary duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0
        superset db upgrade
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true
        superset init
        
        # FIXED: Create persistent DuckDB file with S3 config for Issue #1
        python -c "
import duckdb
conn = duckdb.connect('/app/superset_home/lakehouse.duckdb')
conn.execute('INSTALL httpfs; LOAD httpfs;')
conn.execute('SET s3_endpoint=\\'minio:9000\\';')
conn.execute('SET s3_access_key_id=\\'minio\\';')
conn.execute('SET s3_secret_access_key=\\'minio123\\';')
conn.execute('SET s3_use_ssl=false;')
conn.execute('SET s3_url_style=\\'path\\';')
conn.close()
print('DuckDB 1.3.0 S3 configuration completed')
"
        
        superset run -h 0.0.0.0 -p 8088
    environment:
      SUPERSET_SECRET_KEY: changeme_lakehouse
      DATABASE_URL: postgresql://postgres:lakehouse@postgres:5432/airflow
    ports:
      - "9030:8088"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/superset:/app/superset_home
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 15s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: ${SUPERSET_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${SUPERSET_MEMORY_RESERVATION:-1G}

  # Portainer for Container Management
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: unless-stopped
    ports:
      - "9060:9000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    networks:
      - lakehouse
    deploy:
      resources:
        limits:
          memory: ${PORTAINER_MEMORY_LIMIT:-512M}
        reservations:
          memory: ${PORTAINER_MEMORY_RESERVATION:-128M}

volumes:
  portainer_data:
    driver: local
