version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.10.4-python3.11
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR:-LocalExecutor}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:lakehouse@postgres:5432/airflow
    AIRFLOW__CORE__PARALLELISM: ${AIRFLOW_PARALLELISM:-32}
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: ${AIRFLOW_MAX_ACTIVE_RUNS:-16}
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: ${AIRFLOW_MAX_ACTIVE_TASKS:-16}
    AIRFLOW__SCHEDULER__MAX_THREADS: ${AIRFLOW_SCHEDULER_THREADS:-4}
    AIRFLOW__WEBSERVER__SECRET_KEY: lakehouse
    AIRFLOW__CORE__FERNET_KEY: 'YXNkZmFzZGZhc2RmYXNkZmFzZGZhc2RmYXNkZmFzZGY='
    AIRFLOW__WEBSERVER__WORKERS: ${AIRFLOW_WEBSERVER_WORKERS:-2}
    # Install DuckDB and dependencies
    _PIP_ADDITIONAL_REQUIREMENTS: 'duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0'
  volumes:
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/dags:/opt/airflow/dags
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/logs:/opt/airflow/logs
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy

networks:
  lakehouse:
    driver: bridge

services:
  # PostgreSQL Database
  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: airflow
      POSTGRES_PASSWORD: lakehouse
      POSTGRES_USER: postgres
      # Performance tuning
      POSTGRES_SHARED_BUFFERS: ${POSTGRES_SHARED_BUFFERS:-256MB}
      POSTGRES_EFFECTIVE_CACHE_SIZE: ${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      POSTGRES_MAINTENANCE_WORK_MEM: ${POSTGRES_MAINTENANCE_WORK_MEM:-128MB}
      POSTGRES_CHECKPOINT_COMPLETION_TARGET: ${POSTGRES_CHECKPOINT_COMPLETION_TARGET:-0.9}
      POSTGRES_WAL_BUFFERS: ${POSTGRES_WAL_BUFFERS:-32MB}
      POSTGRES_DEFAULT_STATISTICS_TARGET: ${POSTGRES_DEFAULT_STATISTICS_TARGET:-100}
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/postgres:/var/lib/postgresql/data
    command: >
      postgres
      -c shared_buffers=${POSTGRES_SHARED_BUFFERS:-256MB}
      -c effective_cache_size=${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      -c maintenance_work_mem=${POSTGRES_MAINTENANCE_WORK_MEM:-128MB}
      -c checkpoint_completion_target=${POSTGRES_CHECKPOINT_COMPLETION_TARGET:-0.9}
      -c wal_buffers=${POSTGRES_WAL_BUFFERS:-32MB}
      -c default_statistics_target=${POSTGRES_DEFAULT_STATISTICS_TARGET:-100}
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${POSTGRES_MEMORY_LIMIT:-2G}
        reservations:
          memory: ${POSTGRES_MEMORY_RESERVATION:-512M}

  # MinIO Object Storage
  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
      MINIO_API_REQUESTS_MAX: ${MINIO_API_REQUESTS_MAX:-1000}
      MINIO_API_REQUESTS_DEADLINE: ${MINIO_API_REQUESTS_DEADLINE:-10s}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/minio:/data
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${MINIO_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${MINIO_MEMORY_RESERVATION:-1G}

  # Initialization Service
  lakehouse-init:
    image: alpine:3.18
    depends_on:
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    entrypoint:
      - /bin/sh
      - /host/init-all-in-one.sh
    environment:
      INSTALL_SAMPLES: "true"
      LAKEHOUSE_ROOT: /mnt/lakehouse
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}:/mnt/lakehouse
      - ./init-all-in-one.sh:/host/init-all-in-one.sh:ro
    networks:
      - lakehouse
    restart: "no"

  # Spark Master
  spark-master:
    image: bitnami/spark:3.5.0
    depends_on:
      minio:
        condition: service_healthy
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_MEMORY: ${SPARK_MASTER_MEMORY:-2g}
      SPARK_MASTER_CORES: ${SPARK_MASTER_CORES:-2}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/jobs:/opt/spark/jobs
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/logs:/opt/spark/logs
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${SPARK_MASTER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${SPARK_MASTER_MEMORY_RESERVATION:-1G}

  # Spark Worker
  spark-worker:
    image: bitnami/spark:3.5.0
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-8g}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-4}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/jobs:/opt/spark/jobs
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/logs:/opt/spark/logs
    networks:
      - lakehouse
    restart: unless-stopped
    deploy:
      replicas: ${SPARK_WORKER_INSTANCES:-1}
      resources:
        limits:
          memory: ${SPARK_WORKER_MEMORY_LIMIT:-16G}
          cpus: ${SPARK_WORKER_CPU_LIMIT:-8}
        reservations:
          memory: ${SPARK_WORKER_MEMORY_RESERVATION:-4G}
          cpus: ${SPARK_WORKER_CPU_RESERVATION:-2}

  # JupyterLab
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.5.0
    depends_on:
      spark-master:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: lakehouse
      SPARK_MASTER: spark://spark-master:7077
      # Spark configuration for Jupyter
      SPARK_DRIVER_MEMORY: ${JUPYTER_SPARK_DRIVER_MEMORY:-2g}
      SPARK_EXECUTOR_MEMORY: ${JUPYTER_SPARK_EXECUTOR_MEMORY:-2g}
      SPARK_EXECUTOR_CORES: ${JUPYTER_SPARK_EXECUTOR_CORES:-2}
    ports:
      - "9040:8888"
    user: root
    command: >
      bash -c "
      set -e
      echo 'Installing additional packages...'
      pip install --no-cache-dir duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0 pandas matplotlib seaborn plotly
      
      echo 'Setting up Jupyter configuration...'
      mkdir -p /home/jovyan/.jupyter
      
      echo 'Starting Jupyter...'
      chown -R jovyan:users /home/jovyan
      sudo -u jovyan start-notebook.sh --NotebookApp.token='lakehouse' --NotebookApp.password='' --NotebookApp.allow_origin='*' --NotebookApp.disable_check_xsrf=True
      "
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/notebooks:/home/jovyan/notebooks
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/jupyter-config:/home/jovyan/.jupyter
    networks:
      - lakehouse
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${JUPYTER_MEMORY_LIMIT:-8G}
        reservations:
          memory: ${JUPYTER_MEMORY_RESERVATION:-2G}

  # Airflow Database Initialization
  airflow-init:
    <<: *airflow-common
    depends_on:
      postgres:
        condition: service_healthy
      lakehouse-init:
        condition: service_completed_successfully
    command: >
      bash -c "
      set -e
      echo 'Installing additional packages...'
      pip install --no-cache-dir duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0
      
      echo 'Initializing Airflow database...'
      airflow db init
      
      echo 'Creating admin user...'
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true
      
      echo 'Airflow initialization completed successfully'
      "
    restart: "no"

  # Airflow Scheduler
  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: scheduler
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: ${AIRFLOW_SCHEDULER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${AIRFLOW_SCHEDULER_MEMORY_RESERVATION:-1G}

  # Airflow Webserver
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-scheduler:
        condition: service_started
    command: webserver
    ports:
      - "9020:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${AIRFLOW_WEBSERVER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${AIRFLOW_WEBSERVER_MEMORY_RESERVATION:-1G}

  # Apache Superset - FIXED YAML syntax issue
  superset:
    image: apache/superset:latest
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/bash
      - -c
      - |
        set -e
        echo "Installing DuckDB and dependencies..."
        pip install --no-cache-dir psycopg2-binary duckdb==1.3.0 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==17.0.0 sqlalchemy>=1.4.0
        
        echo "Initializing Superset database..."
        superset db upgrade
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true
        superset init
        
        echo "Configuring DuckDB with S3 settings..."
        python3 << 'EOF'
        import duckdb
        import os
        
        # Ensure directory exists
        os.makedirs('/app/superset_home', exist_ok=True)
        
        # Create and configure DuckDB connection
        conn = duckdb.connect('/app/superset_home/lakehouse.duckdb')
        conn.execute('INSTALL httpfs; LOAD httpfs;')
        conn.execute("SET s3_endpoint='minio:9000';")
        conn.execute("SET s3_access_key_id='minio';") 
        conn.execute("SET s3_secret_access_key='minio123';")
        conn.execute("SET s3_use_ssl=false;")
        conn.execute("SET s3_url_style='path';")
        
        # Persist S3 settings for session consistency
        try:
            conn.execute("CREATE SCHEMA IF NOT EXISTS config;")
            conn.execute("""
                CREATE OR REPLACE MACRO configure_s3() AS (
                    INSTALL httpfs;
                    LOAD httpfs;
                    SET s3_endpoint='minio:9000';
                    SET s3_access_key_id='minio';
                    SET s3_secret_access_key='minio123';
                    SET s3_use_ssl=false;
                    SET s3_url_style='path';
                )
            """)
            print("✅ Created S3 configuration macro")
        except Exception as e:
            print(f"⚠️ Macro creation warning: {e}")
        
        # Test the connection
        try:
            result = conn.execute("SELECT 'DuckDB S3 configuration successful' as status;").fetchone()
            print(f"✅ {result[0]}")
        except Exception as e:
            print(f"⚠️ Configuration warning: {e}")
        finally:
            conn.close()
        EOF
        
        echo "Starting Superset webserver..."
        superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger
    environment:
      SUPERSET_SECRET_KEY: changeme_lakehouse
      DATABASE_URL: postgresql://postgres:lakehouse@postgres:5432/airflow
      SUPERSET_WORKERS: ${SUPERSET_WORKERS:-4}
      SUPERSET_TIMEOUT: ${SUPERSET_TIMEOUT:-60}
    ports:
      - "9030:8088"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/superset:/app/superset_home
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${SUPERSET_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${SUPERSET_MEMORY_RESERVATION:-1G}

  # Portainer for Container Management
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: unless-stopped
    ports:
      - "9060:9000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    networks:
      - lakehouse
    deploy:
      resources:
        limits:
          memory: ${PORTAINER_MEMORY_LIMIT:-512M}
        reservations:
          memory: ${PORTAINER_MEMORY_RESERVATION:-128M}

  # Homer Dashboard (Optional)
  homer:
    image: b4bz/homer:latest
    container_name: homer
    restart: unless-stopped
    ports:
      - "9061:8080"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/homer:/www/assets
    networks:
      - lakehouse
    environment:
      INIT_ASSETS: 1
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 64M

volumes:
  portainer_data:
    driver: local
