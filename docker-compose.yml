# ========================================
# Lakehouse Lab Docker Compose Stack
# ========================================
# 
# ‚ö†Ô∏è  IMPORTANT: Do NOT run 'docker compose up -d' directly for new installations!
# 
# For new installations, use:
#   ./install.sh
# 
# The installer generates secure credentials in .env file that this 
# compose file requires. Running without install.sh will fail with
# "variable not set" errors.
# 
# For existing installations (day-to-day operations):
#   docker compose up -d     # Start services
#   docker compose down      # Stop services
#   docker compose restart   # Restart services
# 
# üîß Service Configuration:
#   Use './scripts/configure-services.sh' to enable/disable services
#   This creates a docker-compose.override.yml file to control which services run
# ========================================

x-airflow-common: &airflow-common
  image: apache/airflow:2.10.3-python3.11
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR:-LocalExecutor}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-lakehouse}
    AIRFLOW__CORE__PARALLELISM: ${AIRFLOW_PARALLELISM:-32}
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: ${AIRFLOW_MAX_ACTIVE_RUNS:-16}
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: ${AIRFLOW_MAX_ACTIVE_TASKS:-16}
    AIRFLOW__SCHEDULER__MAX_THREADS: ${AIRFLOW_SCHEDULER_THREADS:-4}
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
    AIRFLOW__WEBSERVER__WORKERS: ${AIRFLOW_WEBSERVER_WORKERS:-2}
    # Install DuckDB and dependencies
    _PIP_ADDITIONAL_REQUIREMENTS: 'duckdb==1.3.2 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==14.0.2 sqlalchemy>=1.4.0'
  volumes:
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/dags:/opt/airflow/dags
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/logs:/opt/airflow/logs
    - ${LAKEHOUSE_ROOT:-./lakehouse-data}/airflow/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy
  networks:
    - lakehouse

networks:
  lakehouse:
    driver: bridge

services:
  # PostgreSQL Database
  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-lakehouse}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      # Performance tuning
      POSTGRES_SHARED_BUFFERS: ${POSTGRES_SHARED_BUFFERS:-256MB}
      POSTGRES_EFFECTIVE_CACHE_SIZE: ${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      POSTGRES_MAINTENANCE_WORK_MEM: ${POSTGRES_MAINTENANCE_WORK_MEM:-128MB}
      POSTGRES_CHECKPOINT_COMPLETION_TARGET: ${POSTGRES_CHECKPOINT_COMPLETION_TARGET:-0.9}
      POSTGRES_WAL_BUFFERS: ${POSTGRES_WAL_BUFFERS:-32MB}
      POSTGRES_DEFAULT_STATISTICS_TARGET: ${POSTGRES_DEFAULT_STATISTICS_TARGET:-100}
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/postgres:/var/lib/postgresql/data
    # Uncomment to expose PostgreSQL to host (for external tools like pgAdmin, DBeaver, etc.)
    # ports:
    #   - "5432:5432"
    command: >
      postgres
      -c shared_buffers=${POSTGRES_SHARED_BUFFERS:-256MB}
      -c effective_cache_size=${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      -c maintenance_work_mem=${POSTGRES_MAINTENANCE_WORK_MEM:-128MB}
      -c checkpoint_completion_target=${POSTGRES_CHECKPOINT_COMPLETION_TARGET:-0.9}
      -c wal_buffers=${POSTGRES_WAL_BUFFERS:-32MB}
      -c default_statistics_target=${POSTGRES_DEFAULT_STATISTICS_TARGET:-100}
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${POSTGRES_MEMORY_LIMIT:-2G}
        reservations:
          memory: ${POSTGRES_MEMORY_RESERVATION:-512M}

  # MinIO Object Storage
  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_API_REQUESTS_MAX: ${MINIO_API_REQUESTS_MAX:-1000}
      MINIO_API_REQUESTS_DEADLINE: ${MINIO_API_REQUESTS_DEADLINE:-10s}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/minio:/data
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${MINIO_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${MINIO_MEMORY_RESERVATION:-1G}

  # Initialization Service
  lakehouse-init:
    image: alpine:3.18
    depends_on:
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    entrypoint:
      - /bin/sh
      - -c
      - "apk add --no-cache bash && /bin/bash /host/init-all-in-one-modular.sh"
    environment:
      INSTALL_SAMPLES: "true"
      LAKEHOUSE_ROOT: /mnt/lakehouse
      DOCKER_CONTAINER: "true"
      # MinIO credentials for initialization
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      # MinIO readiness configuration (adjust for slower systems)
      MINIO_READY_TIMEOUT: ${MINIO_READY_TIMEOUT:-20}          # attempts (default: 60s total)
      MINIO_READY_INTERVAL: ${MINIO_READY_INTERVAL:-3}         # seconds between attempts
      BUCKET_CREATE_RETRIES: ${BUCKET_CREATE_RETRIES:-8}       # bucket creation retry attempts
      SKIP_MINIO_READY_CHECK: ${SKIP_MINIO_READY_CHECK:-false} # set to 'true' to skip readiness check
      # Host IP for Homer dashboard (detected from host system)
      HOST_IP: ${HOST_IP:-}
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}:/mnt/lakehouse
      - ./init-all-in-one-modular.sh:/host/init-all-in-one-modular.sh:ro
      - ./scripts:/host/scripts:ro
      - ./templates:/host/templates:ro
    networks:
      - lakehouse
    restart: "no"

  # Spark Master
  spark-master:
    image: apache/spark:3.5.6
    depends_on:
      minio:
        condition: service_healthy
    environment:
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/jobs:/opt/spark/work-dir
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/logs:/opt/spark/logs
    networks:
      - lakehouse
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master &
      sleep 10;
      touch /tmp/spark-started;
      wait
      "
    healthcheck:
      test: ["CMD", "test", "-f", "/tmp/spark-started"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${SPARK_MASTER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${SPARK_MASTER_MEMORY_RESERVATION:-1G}

  # Spark Worker
  spark-worker:
    image: apache/spark:3.5.6
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-8g}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-4}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/jobs:/opt/spark/work-dir
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/spark/logs:/opt/spark/logs
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    networks:
      - lakehouse
    restart: unless-stopped
    deploy:
      replicas: ${SPARK_WORKER_INSTANCES:-1}
      resources:
        limits:
          memory: ${SPARK_WORKER_MEMORY_LIMIT:-16G}
          cpus: ${SPARK_WORKER_CPU_LIMIT:-8}
        reservations:
          memory: ${SPARK_WORKER_MEMORY_RESERVATION:-4G}
          cpus: ${SPARK_WORKER_CPU_RESERVATION:-2}

  # JupyterLab
  jupyter:
    image: quay.io/jupyter/pyspark-notebook:latest
    depends_on:
      spark-master:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: ${JUPYTER_TOKEN}
      SPARK_MASTER: spark://spark-master:7077
      # MinIO credentials for automatic notebook configuration
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      # Spark configuration for Jupyter
      SPARK_DRIVER_MEMORY: ${JUPYTER_SPARK_DRIVER_MEMORY:-2g}
      SPARK_EXECUTOR_MEMORY: ${JUPYTER_SPARK_EXECUTOR_MEMORY:-2g}
      SPARK_EXECUTOR_CORES: ${JUPYTER_SPARK_EXECUTOR_CORES:-2}
      # PySpark environment variables
      SPARK_HOME: ${SPARK_HOME:-/usr/local/spark}
      PYTHONPATH: ${PYTHONPATH:-/usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip}
      PYSPARK_PYTHON: /opt/conda/bin/python
      PYSPARK_DRIVER_PYTHON: /opt/conda/bin/python
      PYSPARK_SUBMIT_ARGS: --master spark://spark-master:7077 pyspark-shell
    ports:
      - "9040:8888"
    user: root
    command:
      - bash
      - -c
      - |
        set -e
        echo 'Installing additional packages...'
        # Remove conflicting pyspark-connect first, then install compatible versions
        pip uninstall -y pyspark-connect || echo 'pyspark-connect not found'
        pip install --no-cache-dir pyspark==3.5.0 duckdb==1.3.2 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow 'sqlalchemy>=1.4.0' psycopg2-binary pandas matplotlib seaborn plotly
        
        echo 'Setting up Jupyter configuration...'
        mkdir -p /home/jovyan/.jupyter
        
        echo 'Setting up permissions and environment...'
        chown -R jovyan:users /home/jovyan 2>/dev/null || echo 'Permission setup skipped'
        
        # Install packages for jovyan user as well to ensure availability
        sudo -u jovyan /opt/conda/bin/pip install --user --no-cache-dir pyspark==3.5.0 duckdb==1.3.2 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow pandas matplotlib seaborn plotly 'sqlalchemy>=1.4.0'
        
        # Ensure PySpark environment variables are set for jovyan user
        echo 'export SPARK_HOME=/usr/local/spark' >> /home/jovyan/.bashrc
        echo 'export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH' >> /home/jovyan/.bashrc
        echo 'export PYSPARK_PYTHON=/opt/conda/bin/python' >> /home/jovyan/.bashrc
        echo 'export PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python' >> /home/jovyan/.bashrc
        echo 'export PYSPARK_SUBMIT_ARGS=--master spark://spark-master:7077 pyspark-shell' >> /home/jovyan/.bashrc
        
        # Ensure MinIO credentials are available to jovyan user
        echo "export MINIO_ROOT_USER='${MINIO_ROOT_USER}'" >> /home/jovyan/.bashrc
        echo "export MINIO_ROOT_PASSWORD='${MINIO_ROOT_PASSWORD}'" >> /home/jovyan/.bashrc
        
        echo 'Starting Jupyter...'
        # Use the proper Jupyter startup command with conda environment
        exec sudo -E -u jovyan /opt/conda/bin/jupyter lab --NotebookApp.token='${JUPYTER_TOKEN}' --NotebookApp.password='' --NotebookApp.allow_origin='*' --NotebookApp.disable_check_xsrf=True
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/notebooks:/home/jovyan/notebooks
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/jupyter-config:/home/jovyan/.jupyter
    networks:
      - lakehouse
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${JUPYTER_MEMORY_LIMIT:-8G}
        reservations:
          memory: ${JUPYTER_MEMORY_RESERVATION:-2G}

  # Airflow Database Initialization
  airflow-init:
    <<: *airflow-common
    depends_on:
      postgres:
        condition: service_healthy
      lakehouse-init:
        condition: service_completed_successfully
    command:
      - bash
      - -c
      - |
        set -e
        echo 'üöÄ Starting Airflow initialization...'
        
        # Enhanced PostgreSQL readiness check with multiple validation steps
        echo '‚è≥ Waiting for PostgreSQL to be fully ready...'
        
        # Step 1: Check if PostgreSQL service is responding
        timeout 120 bash -c 'until pg_isready -h postgres -p 5432 -U ${POSTGRES_USER:-postgres}; do echo "Waiting for postgres service..."; sleep 3; done'
        echo '‚úì PostgreSQL service is responding'
        
        # Step 2: Wait for database to be accessible and accept connections
        echo "Attempting to connect to database: ${POSTGRES_DB:-lakehouse} with user: ${POSTGRES_USER:-postgres}"
        echo "Debug: Testing database connection..."
        if PGPASSWORD=${POSTGRES_PASSWORD} psql -h postgres -p 5432 -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-lakehouse} -c "SELECT 1;" 2>&1; then
          echo '‚úì PostgreSQL database is accessible'
        else
          echo "Initial connection failed, waiting for database to be ready..."
          timeout 120 bash -c 'until PGPASSWORD=${POSTGRES_PASSWORD} psql -h postgres -p 5432 -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-lakehouse} -c "SELECT 1;" >/dev/null 2>&1; do echo "Waiting for database accessibility... ($(date))"; sleep 5; done'
          echo '‚úì PostgreSQL database is accessible'
        fi
        
        # Step 3: Additional safety delay to ensure full PostgreSQL initialization
        echo '‚è≥ Adding safety delay for PostgreSQL full initialization...'
        sleep 10
        
        echo 'üì¶ Installing additional packages...'
        pip install --no-cache-dir duckdb==1.3.2 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==14.0.2 'sqlalchemy>=1.4.0'
        
        echo 'üóÑÔ∏è  Initializing Airflow database...'
        # Retry database initialization if it fails initially
        max_retries=3
        retry_count=0
        while [ $$retry_count -lt $$max_retries ]; do
          if airflow db init; then
            echo '‚úÖ Airflow database initialized successfully'
            break
          else
            retry_count=$$((retry_count + 1))
            if [ $$retry_count -lt $$max_retries ]; then
              echo "‚ö†Ô∏è  Database initialization attempt $$retry_count failed, retrying in 10 seconds..."
              sleep 10
            else
              echo "‚ùå Database initialization failed after $$max_retries attempts"
              exit 1
            fi
          fi
        done
        
        echo 'Upgrading database schema...'
        airflow db upgrade
        
        echo 'üë§ Creating admin user...'
        airflow users create --username ${AIRFLOW_ADMIN_USER:-admin} --firstname Admin --lastname User --role Admin --email admin@example.com --password ${AIRFLOW_ADMIN_PASSWORD} || echo 'Admin user already exists'
        
        echo 'üîç Verifying database initialization...'
        airflow db check
        
        echo '‚úÖ Airflow initialization completed successfully!'
    restart: "no"
    healthcheck:
      test: ["CMD-SHELL", "airflow db check || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 180s

  # Airflow Scheduler
  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: scheduler
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: ${AIRFLOW_SCHEDULER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${AIRFLOW_SCHEDULER_MEMORY_RESERVATION:-1G}

  # Airflow Webserver
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      airflow-scheduler:
        condition: service_started
    command: >
      bash -c "
      echo 'Starting Airflow webserver...';
      exec airflow webserver
      "
    ports:
      - "9020:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${AIRFLOW_WEBSERVER_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${AIRFLOW_WEBSERVER_MEMORY_RESERVATION:-1G}

  # Apache Superset - FIXED YAML syntax issue
  superset:
    image: apache/superset:latest
    user: root
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/bash
      - -c
      - |
        set -e
        echo "Installing DuckDB and dependencies..."
        
        # Ensure proper permissions for superset_home directory
        echo 'Setting up Superset home directory permissions...';
        mkdir -p /app/superset_home;
        
        # Set permissions to allow any user to read/write (development environment)
        chmod 777 /app/superset_home;
        
        # Create subdirectories with proper permissions
        mkdir -p /app/superset_home/.local /app/superset_home/.cache;
        chmod -R 777 /app/superset_home/.local /app/superset_home/.cache;
        
        # Install packages system-wide since we're running as root initially
        pip install --no-cache-dir psycopg2-binary duckdb==1.3.2 duckdb-engine==0.17.0 boto3==1.35.5 pyarrow==14.0.2 'sqlalchemy>=1.4.0' Pillow;
        
        echo "Initializing Superset database...";
        superset db upgrade;
        superset fab create-admin --username ${SUPERSET_ADMIN_USER:-admin} --firstname Admin --lastname User --email admin@example.com --password ${SUPERSET_ADMIN_PASSWORD} || true;
        superset init;
        
        # Small delay to ensure database is fully initialized
        sleep 2
        
        echo "Superset ready for manual configuration - see SUPERSET_DATABASE_SETUP.md"
        
        echo "Starting Superset webserver..."
        superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-lakehouse}
      SUPERSET_WORKERS: ${SUPERSET_WORKERS:-4}
      SUPERSET_TIMEOUT: ${SUPERSET_TIMEOUT:-60}
      # Ensure Python finds user-installed packages
      PYTHONPATH: "${PYTHONPATH}:/app/superset_home/.local/lib/python3.9/site-packages:/app/superset_home/.local/lib/python3.10/site-packages:/app/superset_home/.local/lib/python3.11/site-packages"
    ports:
      - "9030:8088"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/superset:/app/superset_home
      - ./scripts:/host/scripts:ro
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: ${SUPERSET_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${SUPERSET_MEMORY_RESERVATION:-1G}

  # Portainer for Container Management
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: unless-stopped
    ports:
      - "9060:9000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    networks:
      - lakehouse
    deploy:
      resources:
        limits:
          memory: ${PORTAINER_MEMORY_LIMIT:-512M}
        reservations:
          memory: ${PORTAINER_MEMORY_RESERVATION:-128M}

  # Homer Dashboard (Optional)
  homer:
    image: b4bz/homer:latest
    container_name: homer
    restart: unless-stopped
    ports:
      - "9061:8080"
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/homer:/www/assets
    networks:
      - lakehouse
    environment:
      INIT_ASSETS: 1
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 64M

  # Vizro Dashboard Framework
  vizro:
    image: python:3.11-slim
    container_name: vizro
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      lakehouse-init:
        condition: service_completed_successfully
    ports:
      - "9050:8050"
    environment:
      VIZRO_SECRET_KEY: ${VIZRO_SECRET_KEY:-vizro-dev-secret}
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-lakehouse}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_ENDPOINT: http://minio:9000
    working_dir: /app
    command: 
      - bash
      - -c
      - |
        pip install --no-cache-dir vizro[default] pandas plotly psycopg2-binary boto3 duckdb pyarrow sqlalchemy
        cd /app/dashboards && python app.py
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/vizro:/app/dashboards
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "bash", "-c", "curl -s -o /dev/null -w '%{http_code}' http://localhost:8050 | grep -E '^[23][0-9][0-9]$' || pgrep -f 'python.*app.py'"]
      interval: 30s
      timeout: 15s
      retries: 8
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: ${VIZRO_MEMORY_LIMIT:-2G}
        reservations:
          memory: ${VIZRO_MEMORY_RESERVATION:-512M}

  # LanceDB Vector Database Service
  lancedb:
    image: python:3.11-slim
    container_name: lancedb
    restart: unless-stopped
    depends_on:
      minio:
        condition: service_healthy
      lakehouse-init:
        condition: service_completed_successfully
    ports:
      - "9080:8000"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_ENDPOINT: http://minio:9000
      LANCE_DATA_DIR: /app/data
    working_dir: /app
    command:
      - bash
      - -c
      - |
        pip install --no-cache-dir lancedb pyarrow pandas fastapi uvicorn boto3 numpy
        python /app/service/lancedb_service.py
    volumes:
      - ${LAKEHOUSE_ROOT:-./lakehouse-data}/lancedb:/app
    networks:
      - lakehouse
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: ${LANCEDB_MEMORY_LIMIT:-3G}
        reservations:
          memory: ${LANCEDB_MEMORY_RESERVATION:-1G}

volumes:
  portainer_data:
    driver: local
