{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - LanceDB Vector Search & AI Workflows\n",
    "\n",
    "This notebook demonstrates how to use LanceDB for vector storage, similarity search, and AI/ML workflows in your lakehouse environment.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to connect to the LanceDB service\n",
    "- Vector storage and retrieval operations\n",
    "- Similarity search and semantic matching\n",
    "- Integration with embeddings and AI models\n",
    "- Building AI-powered search applications\n",
    "- Working with different vector types (documents, images, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package.split('[')[0])\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install LanceDB and related packages\n",
    "install_package('requests')\n",
    "install_package('numpy')\n",
    "install_package('pandas')\n",
    "install_package('scikit-learn')  # For sample embeddings\n",
    "install_package('matplotlib')\n",
    "install_package('seaborn')\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to LanceDB Service\n",
    "\n",
    "The lakehouse stack includes a LanceDB REST API service for vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LanceDB service connection\nLANCEDB_URL = 'http://lancedb:8000'  # Container-to-container connection\n\n# Test connection and get service info\ntry:\n    response = requests.get(f'{LANCEDB_URL}/health', timeout=5)\n    if response.status_code == 200:\n        health_info = response.json()\n        print(\"‚úÖ Connected to LanceDB service!\")\n        print(f\"Status: {health_info['status']}\")\n        print(f\"Available tables: {health_info['tables']}\")\n        print(f\"Data directory: {health_info['data_directory']}\")\n    else:\n        print(f\"‚ùå LanceDB service responded with status {response.status_code}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"‚ùå Could not connect to LanceDB service: {e}\")\n    print(\"üí° Make sure the lakehouse stack is running with: docker compose up -d\")\n\n# Get general service information\ntry:\n    response = requests.get(f'{LANCEDB_URL}/')\n    if response.status_code == 200:\n        service_info = response.json()\n        print(f\"\\nüîß Service Info:\")\n        print(f\"Version: {service_info['version']}\")\n        print(f\"Available endpoints: {list(service_info['endpoints'].keys())}\")\nexcept:\n    pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Available Tables\n",
    "\n",
    "Let's see what vector tables are already available and examine their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available tables\n",
    "def get_tables():\n",
    "    \"\"\"Get list of all available LanceDB tables\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'{LANCEDB_URL}/tables')\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['tables']\n",
    "        else:\n",
    "            print(f\"Error getting tables: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get table information\n",
    "def get_table_info(table_name):\n",
    "    \"\"\"Get detailed information about a specific table\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'{LANCEDB_URL}/tables/{table_name}/info')\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error getting table info: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Explore all tables\n",
    "tables = get_tables()\n",
    "print(f\"üìä Found {len(tables)} tables:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"\\nüóÇÔ∏è  Table: {table['name']}\")\n",
    "    print(f\"   Records: {table['count']:,}\")\n",
    "    \n",
    "    # Get detailed info\n",
    "    info = get_table_info(table['name'])\n",
    "    if info:\n",
    "        print(f\"   Columns: {info['columns']}\")\n",
    "        print(f\"   Version: {info['version']}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        if info['sample_data']:\n",
    "            sample = info['sample_data'][0]\n",
    "            print(f\"   Sample record keys: {list(sample.keys())}\")\n",
    "            if 'text' in sample:\n",
    "                print(f\"   Sample text: '{sample['text'][:60]}...'\")\n",
    "            if 'vector' in sample:\n",
    "                print(f\"   Vector dimension: {len(sample['vector'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Search Operations\n",
    "\n",
    "Let's perform similarity searches on the existing vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search vectors\n",
    "def search_vectors(table_name, query_vector=None, limit=5):\n",
    "    \"\"\"Search for similar vectors in a table\"\"\"\n",
    "    try:\n",
    "        payload = {\n",
    "            'vector': query_vector,\n",
    "            'limit': limit\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f'{LANCEDB_URL}/tables/{table_name}/search', json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Search error: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Let's search the document embeddings table\n",
    "print(\"üîç Searching document embeddings...\")\n",
    "\n",
    "# First, let's get some sample data without a query vector\n",
    "doc_results = search_vectors('document_embeddings', query_vector=None, limit=3)\n",
    "\n",
    "if doc_results:\n",
    "    print(f\"\\nüìù Found {doc_results['count']} sample documents:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, doc in enumerate(doc_results['results'][:3], 1):\n",
    "        print(f\"\\n{i}. Document ID: {doc['id']}\")\n",
    "        print(f\"   Category: {doc['category']}\")\n",
    "        print(f\"   Text: {doc['text'][:100]}...\")\n",
    "        print(f\"   Vector dims: {len(doc['vector'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do a real similarity search\n",
    "# We'll use one of the existing vectors as a query\n",
    "\n",
    "if doc_results and doc_results['results']:\n",
    "    # Use the first document's vector as our query\n",
    "    query_vector = doc_results['results'][0]['vector']\n",
    "    original_text = doc_results['results'][0]['text']\n",
    "    \n",
    "    print(f\"üéØ Searching for documents similar to:\")\n",
    "    print(f\"'{original_text[:80]}...'\")\n",
    "    print()\n",
    "    \n",
    "    # Search for similar documents\n",
    "    similar_docs = search_vectors('document_embeddings', query_vector=query_vector, limit=5)\n",
    "    \n",
    "    if similar_docs:\n",
    "        print(f\"üîç Found {similar_docs['count']} similar documents:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for i, doc in enumerate(similar_docs['results'], 1):\n",
    "            print(f\"\\n{i}. Similarity Score: {doc.get('_distance', 'N/A')}\")\n",
    "            print(f\"   Document ID: {doc['id']}\")\n",
    "            print(f\"   Category: {doc['category']}\")\n",
    "            print(f\"   Text: {doc['text'][:120]}...\")\n",
    "            \n",
    "            # Calculate manual cosine similarity for demonstration\n",
    "            if 'vector' in doc:\n",
    "                similarity = cosine_similarity([query_vector], [doc['vector']])[0][0]\n",
    "                print(f\"   Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Image Embeddings\n",
    "\n",
    "Let's explore the image embeddings table and perform visual similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore image embeddings\n",
    "print(\"üñºÔ∏è  Exploring image embeddings...\")\n",
    "\n",
    "image_results = search_vectors('image_embeddings', query_vector=None, limit=5)\n",
    "\n",
    "if image_results:\n",
    "    print(f\"\\nüìä Found {image_results['count']} sample images:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    image_data = []\n",
    "    \n",
    "    for i, img in enumerate(image_results['results'], 1):\n",
    "        print(f\"\\n{i}. Image: {img['filename']}\")\n",
    "        print(f\"   Tags: {', '.join(img['tags'])}\")\n",
    "        print(f\"   Vector dims: {len(img['vector'])}\")\n",
    "        \n",
    "        # Parse metadata\n",
    "        metadata = json.loads(img['metadata']) if isinstance(img['metadata'], str) else img['metadata']\n",
    "        print(f\"   Dimensions: {metadata.get('width', 'N/A')}x{metadata.get('height', 'N/A')}\")\n",
    "        \n",
    "        image_data.append({\n",
    "            'id': img['id'],\n",
    "            'filename': img['filename'],\n",
    "            'tags': ', '.join(img['tags']),\n",
    "            'vector_dim': len(img['vector']),\n",
    "            'width': metadata.get('width', 0),\n",
    "            'height': metadata.get('height', 0)\n",
    "        })\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    if image_data:\n",
    "        df_images = pd.DataFrame(image_data)\n",
    "        print(f\"\\nüìà Image Collection Summary:\")\n",
    "        print(df_images.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating Custom Embeddings\n",
    "\n",
    "Let's create our own embeddings and add them to a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample lakehouse-related documents for embedding\n",
    "sample_texts = [\n",
    "    \"Data lakehouse architecture combines the best of data warehouses and data lakes\",\n",
    "    \"Apache Spark provides distributed computing for big data processing\",\n",
    "    \"MinIO offers S3-compatible object storage for cloud-native applications\",\n",
    "    \"PostgreSQL is a powerful relational database for analytics workloads\", \n",
    "    \"Apache Airflow orchestrates complex data workflows and ETL pipelines\",\n",
    "    \"Jupyter notebooks enable interactive data science and exploration\",\n",
    "    \"Apache Superset provides modern business intelligence dashboards\",\n",
    "    \"Vector databases enable semantic search and AI-powered applications\",\n",
    "    \"Machine learning models require feature stores and model registries\",\n",
    "    \"Real-time streaming data processing using Apache Kafka and Spark\",\n",
    "    \"Data quality monitoring and observability in modern data stacks\",\n",
    "    \"Cloud-native data platforms with Kubernetes orchestration\"\n",
    "]\n",
    "\n",
    "print(f\"üìù Created {len(sample_texts)} sample documents\")\n",
    "\n",
    "# Create TF-IDF embeddings (simple example - in production you'd use more sophisticated embeddings)\n",
    "vectorizer = TfidfVectorizer(max_features=128, stop_words='english')\n",
    "tfidf_vectors = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "# Convert to dense numpy arrays\n",
    "embeddings = tfidf_vectors.toarray()\n",
    "\n",
    "print(f\"‚úÖ Generated {embeddings.shape[0]} embeddings with {embeddings.shape[1]} dimensions\")\n",
    "print(f\"Feature words: {vectorizer.get_feature_names_out()[:10]}...\")\n",
    "\n",
    "# Prepare data for LanceDB\n",
    "lakehouse_docs = []\n",
    "for i, (text, embedding) in enumerate(zip(sample_texts, embeddings)):\n",
    "    lakehouse_docs.append({\n",
    "        'id': i + 100,  # Start with ID 100 to avoid conflicts\n",
    "        'text': text,\n",
    "        'vector': embedding.tolist(),\n",
    "        'category': 'lakehouse-guide',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'source': 'notebook-generated'\n",
    "    })\n",
    "\n",
    "print(f\"üìã Prepared {len(lakehouse_docs)} documents for insertion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table for our custom embeddings\n",
    "def create_table(table_name, table_type='custom'):\n",
    "    \"\"\"Create a new LanceDB table\"\"\"\n",
    "    try:\n",
    "        payload = {'type': table_type}\n",
    "        response = requests.post(f'{LANCEDB_URL}/tables/{table_name}/create', json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            result = response.json()\n",
    "            if 'already exists' in result.get('message', ''):\n",
    "                print(f\"‚ÑπÔ∏è  Table '{table_name}' already exists\")\n",
    "                return result\n",
    "            else:\n",
    "                print(f\"Error creating table: {response.status_code}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to insert data into a table\n",
    "def insert_data(table_name, records):\n",
    "    \"\"\"Insert records into a LanceDB table\"\"\"\n",
    "    try:\n",
    "        payload = {'records': records}\n",
    "        response = requests.post(f'{LANCEDB_URL}/tables/{table_name}/insert', json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Insert error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create table for lakehouse documentation\n",
    "table_name = 'lakehouse_docs'\n",
    "print(f\"üóÇÔ∏è  Creating table: {table_name}\")\n",
    "\n",
    "create_result = create_table(table_name)\n",
    "if create_result:\n",
    "    print(f\"‚úÖ Table created successfully\")\n",
    "    \n",
    "    # Insert our custom embeddings\n",
    "    print(f\"üì• Inserting {len(lakehouse_docs)} documents...\")\n",
    "    \n",
    "    insert_result = insert_data(table_name, lakehouse_docs)\n",
    "    if insert_result:\n",
    "        print(f\"‚úÖ Inserted {insert_result['inserted_count']} records\")\n",
    "        print(f\"   Total records in table: {insert_result['total_count']}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to insert data\")\nelse:\n",
    "    print(\"‚ùå Failed to create table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search Demo\n",
    "\n",
    "Now let's perform semantic searches on our custom lakehouse documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create query embeddings\n",
    "def create_query_embedding(query_text):\n",
    "    \"\"\"Create embedding for a query text\"\"\"\n",
    "    # Transform the query using our trained vectorizer\n",
    "    query_vector = vectorizer.transform([query_text])\n",
    "    return query_vector.toarray()[0].tolist()\n",
    "\n",
    "# Function to perform semantic search\n",
    "def semantic_search(query_text, table_name='lakehouse_docs', limit=3):\n",
    "    \"\"\"Perform semantic search for a query\"\"\"\n",
    "    print(f\"üîç Searching for: '{query_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_embedding = create_query_embedding(query_text)\n",
    "    \n",
    "    # Search the database\n",
    "    results = search_vectors(table_name, query_vector=query_embedding, limit=limit)\n",
    "    \n",
    "    if results and results['results']:\n",
    "        print(f\"Found {len(results['results'])} relevant documents:\\n\")\n",
    "        \n",
    "        for i, doc in enumerate(results['results'], 1):\n",
    "            # Calculate similarity score\n",
    "            similarity = cosine_similarity([query_embedding], [doc['vector']])[0][0]\n",
    "            \n",
    "            print(f\"{i}. Similarity: {similarity:.4f}\")\n",
    "            print(f\"   Text: {doc['text']}\")\n",
    "            print(f\"   Category: {doc['category']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No results found\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different semantic searches\n",
    "search_queries = [\n",
    "    \"distributed computing and big data\",\n",
    "    \"object storage for cloud applications\", \n",
    "    \"workflow orchestration and ETL\",\n",
    "    \"interactive data analysis notebooks\",\n",
    "    \"AI and machine learning infrastructure\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Performing semantic searches on lakehouse documentation\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in search_queries[:2]:  # Test first 2 queries\n",
    "    semantic_search(query, limit=2)\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vector Analytics and Visualization\n",
    "\n",
    "Let's analyze and visualize our vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all documents from our custom table\n",
    "all_docs = search_vectors('lakehouse_docs', query_vector=None, limit=50)\n",
    "\n",
    "if all_docs and all_docs['results']:\n",
    "    # Extract vectors and metadata\n",
    "    vectors = np.array([doc['vector'] for doc in all_docs['results']])\n",
    "    texts = [doc['text'][:30] + '...' for doc in all_docs['results']]\n",
    "    \n",
    "    print(f\"üìä Analyzing {len(vectors)} document vectors\")\n",
    "    print(f\"Vector dimension: {vectors.shape[1]}\")\n",
    "    \n",
    "    # Calculate pairwise similarities\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Similarity heatmap\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                annot=False, \n",
    "                cmap='viridis', \n",
    "                ax=axes[0,0],\n",
    "                cbar_kws={'label': 'Cosine Similarity'})\n",
    "    axes[0,0].set_title('Document Similarity Matrix')\n",
    "    axes[0,0].set_xlabel('Document Index')\n",
    "    axes[0,0].set_ylabel('Document Index')\n",
    "    \n",
    "    # 2. Vector magnitude distribution\n",
    "    vector_norms = np.linalg.norm(vectors, axis=1)\n",
    "    axes[0,1].hist(vector_norms, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,1].set_title('Distribution of Vector Magnitudes')\n",
    "    axes[0,1].set_xlabel('L2 Norm')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Average similarity by document\n",
    "    avg_similarities = np.mean(similarity_matrix, axis=1)\n",
    "    doc_indices = range(len(avg_similarities))\n",
    "    axes[1,0].bar(doc_indices, avg_similarities, alpha=0.7, color='coral')\n",
    "    axes[1,0].set_title('Average Similarity Score by Document')\n",
    "    axes[1,0].set_xlabel('Document Index')\n",
    "    axes[1,0].set_ylabel('Average Similarity')\n",
    "    \n",
    "    # 4. Feature importance (top TF-IDF terms)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    avg_tfidf_scores = np.mean(vectors, axis=0)\n",
    "    top_features_idx = np.argsort(avg_tfidf_scores)[-10:]\n",
    "    top_features = [feature_names[i] for i in top_features_idx]\n",
    "    top_scores = avg_tfidf_scores[top_features_idx]\n",
    "    \n",
    "    axes[1,1].barh(top_features, top_scores, alpha=0.7, color='lightgreen')\n",
    "    axes[1,1].set_title('Top 10 Important Terms (Avg TF-IDF)')\n",
    "    axes[1,1].set_xlabel('Average TF-IDF Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"\\nüìà Vector Analytics:\")\n",
    "    print(f\"   Average similarity: {np.mean(similarity_matrix):.4f}\")\n",
    "    print(f\"   Max similarity: {np.max(similarity_matrix[similarity_matrix < 1.0]):.4f}\")\n",
    "    print(f\"   Min similarity: {np.min(similarity_matrix):.4f}\")\n",
    "    print(f\"   Vector norm range: {np.min(vector_norms):.4f} - {np.max(vector_norms):.4f}\")\n",
    "else:\n",
    "    print(\"No vector data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Use Cases\n",
    "\n",
    "### A. Document Clustering\n",
    "Group similar documents together using vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if 'vectors' in locals() and len(vectors) > 5:\n",
    "    # Perform K-means clustering\n",
    "    n_clusters = min(4, len(vectors))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(vectors)\n",
    "    \n",
    "    # Reduce dimensionality for visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    vectors_2d = pca.fit_transform(vectors)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], \n",
    "                         c=clusters, cmap='tab10', alpha=0.7, s=100)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, txt in enumerate(texts):\n",
    "        plt.annotate(f\"{i}: {txt[:20]}...\", \n",
    "                    (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=8, alpha=0.7)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title('Document Clustering in 2D PCA Space')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print cluster analysis\n",
    "    print(f\"üìä Cluster Analysis ({n_clusters} clusters):\")\n",
    "    for i in range(n_clusters):\n",
    "        cluster_docs = [texts[j] for j in range(len(texts)) if clusters[j] == i]\n",
    "        print(f\"\\nCluster {i} ({len(cluster_docs)} documents):\")\n",
    "        for doc in cluster_docs[:3]:  # Show first 3 docs per cluster\n",
    "            print(f\"  ‚Ä¢ {doc}\")\n",
    "\nelse:\n",
    "    print(\"Need more vector data for clustering analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Recommendation System\n",
    "Build a simple content recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_content(query_text, exclude_exact_match=True, top_k=3):\n",
    "    \"\"\"\n",
    "    Recommend similar content based on a query\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Content recommendations for: '{query_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get recommendations from our vector database\n",
    "    recommendations = semantic_search(query_text, limit=top_k + 1)\n",
    "    \n",
    "    if recommendations and recommendations['results']:\n",
    "        print(\"\\nüí° You might also be interested in:\")\n",
    "        \n",
    "        filtered_results = recommendations['results']\n",
    "        if exclude_exact_match:\n",
    "            # Remove exact matches\n",
    "            filtered_results = [r for r in filtered_results if r['text'].lower() != query_text.lower()]\n",
    "        \n",
    "        for i, rec in enumerate(filtered_results[:top_k], 1):\n",
    "            # Calculate similarity\n",
    "            query_embed = create_query_embedding(query_text)\n",
    "            similarity = cosine_similarity([query_embed], [rec['vector']])[0][0]\n",
    "            \n",
    "            print(f\"\\n{i}. {rec['text']}\")\n",
    "            print(f\"   üìä Relevance: {similarity:.3f}\")\n",
    "            print(f\"   üè∑Ô∏è  Category: {rec['category']}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test the recommendation system\n",
    "test_queries = [\n",
    "    \"I need help with data processing\",\n",
    "    \"How do I store large datasets\",\n",
    "    \"Building dashboards for analytics\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    recommend_similar_content(query, top_k=2)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration with Lakehouse Ecosystem\n",
    "\n",
    "### Connecting Vector Search with Other Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a comprehensive search function that could integrate with other services\n",
    "def lakehouse_intelligent_search(user_query, search_types=['vector', 'sql']):\n",
    "    \"\"\"\n",
    "    Intelligent search across multiple lakehouse components\n",
    "    \"\"\"\n",
    "    print(f\"üîç Lakehouse Intelligence Search: '{user_query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Vector-based semantic search\n",
    "    if 'vector' in search_types:\n",
    "        print(\"\\nüìä Vector Search Results:\")\n",
    "        vector_results = semantic_search(user_query, limit=2)\n",
    "        results['vector'] = vector_results\n",
    "    \n",
    "    # 2. Could integrate with SQL search (example structure)\n",
    "    if 'sql' in search_types:\n",
    "        print(\"\\nüóÉÔ∏è  Database Search Suggestions:\")\n",
    "        # In a real implementation, this would query PostgreSQL\n",
    "        sql_suggestions = [\n",
    "            \"SELECT * FROM orders WHERE description ILIKE '%data%'\",\n",
    "            \"SELECT service_name, count(*) FROM metrics GROUP BY service_name\"\n",
    "        ]\n",
    "        \n",
    "        for i, suggestion in enumerate(sql_suggestions, 1):\n",
    "            print(f\"   {i}. {suggestion}\")\n",
    "        results['sql_suggestions'] = sql_suggestions\n",
    "    \n",
    "    # 3. Could integrate with MinIO object search\n",
    "    print(\"\\nüìÅ Object Storage Suggestions:\")\n",
    "    storage_suggestions = [\n",
    "        \"s3://lakehouse/raw-data/sample_orders.csv\",\n",
    "        \"s3://processed-data/analytics/monthly_reports/\"\n",
    "    ]\n",
    "    \n",
    "    for i, obj in enumerate(storage_suggestions, 1):\n",
    "        print(f\"   {i}. {obj}\")\n",
    "    results['storage'] = storage_suggestions\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the intelligent search\n",
    "search_result = lakehouse_intelligent_search(\"data processing workflows\")\n",
    "\n",
    "print(\"\\n\\nüí° This demonstrates how LanceDB can be integrated with:\")\n",
    "print(\"   ‚Ä¢ PostgreSQL for structured data queries\")\n",
    "print(\"   ‚Ä¢ MinIO for object storage search\")\n",
    "print(\"   ‚Ä¢ Airflow for workflow recommendations\")\n",
    "print(\"   ‚Ä¢ Jupyter for notebook suggestions\")\n",
    "print(\"   ‚Ä¢ Superset for dashboard templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance and Monitoring\n",
    "\n",
    "Monitor your vector operations and optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance testing function\n",
    "def benchmark_vector_search(table_name='document_embeddings', num_searches=5):\n",
    "    \"\"\"\n",
    "    Benchmark vector search performance\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Benchmarking LanceDB performance ({num_searches} searches)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get a sample vector for searching\n",
    "    sample_data = search_vectors(table_name, limit=1)\n",
    "    if not sample_data or not sample_data['results']:\n",
    "        print(\"No data available for benchmarking\")\n",
    "        return\n",
    "    \n",
    "    query_vector = sample_data['results'][0]['vector']\n",
    "    \n",
    "    # Perform benchmark searches\n",
    "    search_times = []\n",
    "    \n",
    "    for i in range(num_searches):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform search\n",
    "        results = search_vectors(table_name, query_vector=query_vector, limit=5)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        search_time = end_time - start_time\n",
    "        search_times.append(search_time)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"Search {i+1}: {search_time:.3f}s ({results['count']} results)\")\n",
    "        else:\n",
    "            print(f\"Search {i+1}: {search_time:.3f}s (failed)\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if search_times:\n",
    "        avg_time = np.mean(search_times)\n",
    "        min_time = np.min(search_times)\n",
    "        max_time = np.max(search_times)\n",
    "        \n",
    "        print(f\"\\nüìä Performance Summary:\")\n",
    "        print(f\"   Average search time: {avg_time:.3f}s\")\n",
    "        print(f\"   Fastest search: {min_time:.3f}s\")\n",
    "        print(f\"   Slowest search: {max_time:.3f}s\")\n",
    "        print(f\"   Searches per second: {1/avg_time:.1f}\")\n",
    "        \n",
    "        # Visualize performance\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(range(1, len(search_times) + 1), search_times, 'bo-', alpha=0.7)\n",
    "        plt.axhline(y=avg_time, color='r', linestyle='--', alpha=0.7, label=f'Average: {avg_time:.3f}s')\n",
    "        plt.title('Vector Search Performance')\n",
    "        plt.xlabel('Search Number')\n",
    "        plt.ylabel('Response Time (seconds)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_vector_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Practices & Next Steps\n",
    "\n",
    "### üéØ **Production Tips:**\n",
    "\n",
    "1. **Vector Quality**: Use high-quality embeddings (OpenAI, Sentence-BERT, etc.)\n",
    "2. **Indexing**: Consider FAISS or Annoy for large-scale similarity search\n",
    "3. **Caching**: Cache frequently accessed vectors\n",
    "4. **Monitoring**: Track search performance and accuracy\n",
    "5. **Backup**: Regularly backup your vector data\n",
    "\n",
    "### üöÄ **Advanced Use Cases:**\n",
    "\n",
    "1. **RAG Systems**: Retrieval-Augmented Generation with LLMs\n",
    "2. **Recommendation Engines**: Product/content recommendations\n",
    "3. **Image Search**: Visual similarity search for images\n",
    "4. **Anomaly Detection**: Find outliers in vector space\n",
    "5. **Clustering**: Group similar items automatically\n",
    "\n",
    "### üîó **Integration Opportunities:**\n",
    "\n",
    "- **Airflow**: Automate embedding generation pipelines\n",
    "- **Superset**: Create vector search dashboards\n",
    "- **PostgreSQL**: Hybrid vector + relational queries\n",
    "- **MinIO**: Store and version vector models\n",
    "- **Jupyter**: Interactive vector analysis workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary and service information\nprint(\"üéâ LanceDB Vector Search Tutorial Complete!\")\nprint(\"=\" * 50)\nprint(f\"\\nüîó **Access your LanceDB service:**\")\nprint(f\"   ‚Ä¢ API: http://lancedb:8000 (from notebook containers)\")\nprint(f\"   ‚Ä¢ External API: http://localhost:9080 (from host)\")\nprint(f\"   ‚Ä¢ Documentation: http://localhost:9080/docs\")\nprint(f\"   ‚Ä¢ Health check: http://localhost:9080/health\")\n\nprint(f\"\\nüìä **What we've covered:**\")\nprint(f\"   ‚úÖ Connected to LanceDB service\")\nprint(f\"   ‚úÖ Explored existing vector tables\")\nprint(f\"   ‚úÖ Performed similarity searches\")\nprint(f\"   ‚úÖ Created custom embeddings\")\nprint(f\"   ‚úÖ Built semantic search functionality\")\nprint(f\"   ‚úÖ Analyzed vector data with visualizations\")\nprint(f\"   ‚úÖ Implemented clustering and recommendations\")\nprint(f\"   ‚úÖ Benchmarked performance\")\n\nprint(f\"\\nüéØ **Next steps:**\")\nprint(f\"   1. Explore the LanceDB API docs at /docs endpoint\")\nprint(f\"   2. Integrate vector search with your applications\")\nprint(f\"   3. Try other notebook tutorials in the lakehouse lab\")\nprint(f\"   4. Build production-ready embedding pipelines\")\n\nprint(f\"\\n---\")\nprint(f\"üè† **Lakehouse Lab** - AI-Powered Analytics Platform\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}