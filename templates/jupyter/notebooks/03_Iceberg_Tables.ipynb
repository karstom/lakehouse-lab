{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iceberg-intro",
   "metadata": {},
   "source": [
    "# Apache Iceberg Tables in Lakehouse Lab\n",
    "\n",
    "This notebook demonstrates Apache Iceberg table format features:\n",
    "- Time travel queries\n",
    "- Schema evolution\n",
    "- ACID transactions\n",
    "- Snapshot management\n",
    "\n",
    "**Prerequisites:** This notebook requires the Iceberg configuration (`--iceberg` flag during installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iceberg-setup",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\n# Check for Iceberg JAR directory in multiple possible locations\npossible_iceberg_dirs = [\n    \"/home/jovyan/work/iceberg-jars\",  # Mounted when using --iceberg flag\n    \"/opt/bitnami/spark/jars/iceberg\", # Alternative mount location\n    \"/opt/bitnami/spark/jars\"          # Check default Spark jars directory\n]\n\nall_jars = []\niceberg_dir = None\n\nprint(\"üîç Searching for Iceberg JARs...\")\nfor check_dir in possible_iceberg_dirs:\n    print(f\"Checking: {check_dir}\")\n    if os.path.exists(check_dir):\n        jar_files = [f for f in os.listdir(check_dir) if f.endswith('.jar')]\n        \n        # Look for all required JAR files\n        required_jars = [\n            'iceberg-spark-runtime',\n            'iceberg-aws',\n            'hadoop-aws',\n            'aws-java-sdk-bundle',  # AWS SDK v1 (for hadoop-aws compatibility)\n            'bundle-2.',            # AWS SDK v2 bundle (for iceberg-aws)\n            'url-connection-client' # AWS SDK v2 HTTP client\n        ]\n        \n        found_jars = []\n        for jar_name in required_jars:\n            matching_jars = [f for f in jar_files if jar_name in f.lower()]\n            if matching_jars:\n                found_jars.extend(matching_jars)\n        \n        if found_jars:\n            iceberg_dir = check_dir\n            for jar in found_jars:\n                all_jars.append(os.path.join(check_dir, jar))\n            print(f\"‚úÖ Found {len(found_jars)} required JAR(s) in {check_dir}:\")\n            for jar in found_jars:\n                print(f\"   - {jar}\")\n            break\n        else:\n            print(f\"   Directory exists but missing required JARs (has {len(jar_files)} total JARs)\")\n    else:\n        print(f\"   Directory not found\")\n\nif not all_jars:\n    print(\"\\n‚ùå No required JAR files found in any location!\")\n    print(\"\\nüîß This usually means:\")\n    print(\"   1. Installation was not run with --iceberg flag\")\n    print(\"   2. Docker Compose not started with iceberg configuration\")\n    print(\"   3. init-compute.sh failed to download required JARs\")\n    print(\"\\nüí° To fix this:\")\n    print(\"   1. Stop containers: docker compose down\")\n    print(\"   2. Run: ./install.sh --iceberg --fat-server --branch dev\")\n    print(\"   3. This will download JARs and configure Iceberg support\")\n    print(\"\\n‚ö†Ô∏è  Iceberg functionality will not be available without these JARs\")\n    raise FileNotFoundError(\"No required JAR files found - run installation with --iceberg flag\")\n\nprint(f\"\\n‚úÖ Using JAR files from: {iceberg_dir}\")\nprint(f\"Total JARs loaded: {len(all_jars)}\")\n\n# Stop any existing Spark session\ntry:\n    spark.stop()\n    print(\"üîÑ Stopped existing Spark session\")\nexcept:\n    pass\n\n# Configure Spark with Iceberg support and all discovered JARs\nspark = SparkSession.builder \\\n    .appName(\"Lakehouse Lab - Iceberg Demo\") \\\n    .config(\"spark.jars\", \",\".join(all_jars)) \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://lakehouse/iceberg-warehouse/\") \\\n    .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get('MINIO_ROOT_USER', 'minio')) \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get('MINIO_ROOT_PASSWORD', 'minio123')) \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"1\") \\\n    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"5000\") \\\n    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"10000\") \\\n    .getOrCreate()\n\nprint(\"‚úÖ Spark session with Iceberg support initialized!\")\nprint(f\"Spark version: {spark.version}\")\nprint(\"üßä JAR files loaded:\")\nfor jar in all_jars:\n    print(f\"   - {os.path.basename(jar)}\")\n\n# Test S3A connectivity\ntry:\n    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg\")\n    print(\"‚úÖ Iceberg namespace ready\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Warning during namespace creation: {e}\")\n    print(\"This might be normal for first-time setup\")"
  },
  {
   "cell_type": "markdown",
   "id": "create-iceberg-table",
   "metadata": {},
   "source": [
    "## 1. Create an Iceberg Table\n",
    "\n",
    "Let's create a sample Iceberg table with customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initial customer data\n",
    "customers_data = [\n",
    "    (1, \"Alice Johnson\", \"alice@email.com\", \"2023-01-15\", \"Premium\"),\n",
    "    (2, \"Bob Smith\", \"bob@email.com\", \"2023-02-20\", \"Standard\"),\n",
    "    (3, \"Carol Davis\", \"carol@email.com\", \"2023-03-10\", \"Premium\"),\n",
    "    (4, \"David Wilson\", \"david@email.com\", \"2023-04-05\", \"Standard\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"signup_date\", StringType(), False),\n",
    "    StructField(\"tier\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(customers_data, schema)\n",
    "\n",
    "# Create Iceberg table\n",
    "df.writeTo(\"iceberg.customers\").create()\n",
    "\n",
    "print(\"‚úÖ Created Iceberg table 'iceberg.customers'\")\n",
    "spark.sql(\"SELECT * FROM iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-travel-demo",
   "metadata": {},
   "source": [
    "## 2. Time Travel Queries\n",
    "\n",
    "Iceberg allows you to query data as it existed at any point in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current snapshot information\n",
    "snapshots = spark.sql(\"SELECT * FROM iceberg.customers.snapshots\")\n",
    "print(\"üì∏ Table snapshots:\")\n",
    "snapshots.select(\"snapshot_id\", \"timestamp_ms\", \"operation\").show()\n",
    "\n",
    "# Store first snapshot ID for time travel\n",
    "first_snapshot = snapshots.first()[\"snapshot_id\"]\n",
    "print(f\"First snapshot ID: {first_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more data to demonstrate time travel\n",
    "new_customers = [\n",
    "    (5, \"Eve Brown\", \"eve@email.com\", \"2024-01-15\", \"Premium\"),\n",
    "    (6, \"Frank Miller\", \"frank@email.com\", \"2024-02-20\", \"Standard\")\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_customers, schema)\n",
    "new_df.writeTo(\"iceberg.customers\").append()\n",
    "\n",
    "print(\"‚úÖ Added new customers\")\n",
    "print(\"Current data:\")\n",
    "spark.sql(\"SELECT COUNT(*) as current_count FROM iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-historical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query historical data using snapshot ID\n",
    "print(\"üï∞Ô∏è Time travel query - data at first snapshot:\")\n",
    "historical_query = f\"SELECT COUNT(*) as historical_count FROM iceberg.customers VERSION AS OF {first_snapshot}\"\n",
    "spark.sql(historical_query).show()\n",
    "\n",
    "print(\"Comparison:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    'Current' as timepoint, COUNT(*) as record_count \n",
    "FROM iceberg.customers\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Historical' as timepoint, COUNT(*) as record_count \n",
    "FROM iceberg.customers VERSION AS OF \"\"\" + str(first_snapshot) + \"\"\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-evolution",
   "metadata": {},
   "source": [
    "## 3. Schema Evolution\n",
    "\n",
    "Iceberg supports schema evolution without breaking existing queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evolve-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the table\n",
    "spark.sql(\"ALTER TABLE iceberg.customers ADD COLUMN phone STRING\")\n",
    "\n",
    "print(\"‚úÖ Added 'phone' column to table\")\n",
    "print(\"Updated schema:\")\n",
    "spark.sql(\"DESCRIBE iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert-evolved-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data with the new column\n",
    "evolved_customers = [\n",
    "    (7, \"Grace Lee\", \"grace@email.com\", \"2024-03-15\", \"Premium\", \"+1-555-0123\")\n",
    "]\n",
    "\n",
    "evolved_schema = schema.add(StructField(\"phone\", StringType(), True))\n",
    "evolved_df = spark.createDataFrame(evolved_customers, evolved_schema)\n",
    "evolved_df.writeTo(\"iceberg.customers\").append()\n",
    "\n",
    "print(\"‚úÖ Inserted data with new schema\")\n",
    "spark.sql(\"SELECT * FROM iceberg.customers WHERE phone IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-maintenance",
   "metadata": {},
   "source": [
    "## 4. Table Maintenance\n",
    "\n",
    "Iceberg provides operations for managing table snapshots and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table history\n",
    "print(\"üìã Table history:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.customers.history\").show()\n",
    "\n",
    "print(\"üìä Current snapshots:\")\n",
    "spark.sql(\"SELECT snapshot_id, timestamp_ms, operation, summary FROM iceberg.customers.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table files\n",
    "print(\"üìÅ Table files:\")\n",
    "files_df = spark.sql(\"SELECT file_path, file_format, record_count FROM iceberg.customers.files\")\n",
    "files_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollback-demo",
   "metadata": {},
   "source": [
    "## 5. Rollback Capability\n",
    "\n",
    "Iceberg allows you to rollback to previous snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current count\n",
    "print(\"Before rollback:\")\n",
    "spark.sql(\"SELECT COUNT(*) as count FROM iceberg.customers\").show()\n",
    "\n",
    "# Rollback to first snapshot\n",
    "rollback_sql = f\"CALL iceberg.system.rollback_to_snapshot('iceberg.customers', {first_snapshot})\"\n",
    "spark.sql(rollback_sql)\n",
    "\n",
    "print(\"‚úÖ Rolled back to first snapshot\")\n",
    "print(\"After rollback:\")\n",
    "spark.sql(\"SELECT COUNT(*) as count FROM iceberg.customers\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "This notebook demonstrated key Apache Iceberg features:\n",
    "\n",
    "‚úÖ **ACID Transactions** - All operations are atomic and consistent\n",
    "\n",
    "‚úÖ **Time Travel** - Query data as it existed at any point in time\n",
    "\n",
    "‚úÖ **Schema Evolution** - Add columns without breaking existing queries\n",
    "\n",
    "‚úÖ **Snapshot Management** - View and manage table versions\n",
    "\n",
    "‚úÖ **Rollback Capability** - Easily revert to previous states\n",
    "\n",
    "### Next Steps:\n",
    "- Explore partition evolution with `ALTER TABLE ... REPLACE PARTITION FIELD`\n",
    "- Set up branch and tag management for complex workflows\n",
    "- Integrate with Spark streaming for real-time Iceberg updates\n",
    "- Use Iceberg tables in your production analytics pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# spark.sql(\"DROP TABLE iceberg.customers\")\n",
    "# print(\"üßπ Cleaned up demo table\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}