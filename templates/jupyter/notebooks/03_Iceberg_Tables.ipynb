{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iceberg-intro",
   "metadata": {},
   "source": [
    "# Apache Iceberg Tables in Lakehouse Lab\n",
    "\n",
    "This notebook demonstrates Apache Iceberg table format features:\n",
    "- Time travel queries\n",
    "- Schema evolution\n",
    "- ACID transactions\n",
    "- Snapshot management\n",
    "\n",
    "**Prerequisites:** This notebook requires the Iceberg configuration (`--iceberg` flag during installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iceberg-setup",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\n# Get Iceberg JAR paths\niceberg_jars = [\n    \"/home/jovyan/work/iceberg-jars/iceberg-spark-runtime-3.5_2.12-1.5.0.jar\",\n    \"/home/jovyan/work/iceberg-jars/iceberg-aws-1.5.0.jar\"\n]\n\n# Check if JAR files exist\nmissing_jars = []\nfor jar in iceberg_jars:\n    if not os.path.exists(jar):\n        missing_jars.append(jar)\n\nif missing_jars:\n    print(\"‚ùå Missing Iceberg JAR files:\")\n    for jar in missing_jars:\n        print(f\"   - {jar}\")\n    print(\"\\\\nPlease ensure you ran installation with --iceberg flag and that init-compute.sh completed successfully.\")\n    raise FileNotFoundError(\"Required Iceberg JAR files not found\")\n\n# Configure Spark with Iceberg support and proper JARs\nspark = SparkSession.builder \\\\\n    .appName(\"Lakehouse Lab - Iceberg Demo\") \\\\\n    .config(\"spark.jars\", \",\".join(iceberg_jars)) \\\\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\\\n    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n    .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\") \\\\\n    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://lakehouse/iceberg-warehouse/\") \\\\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\\\n    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get('MINIO_ROOT_USER', 'minio')) \\\\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get('MINIO_ROOT_PASSWORD', 'minio123')) \\\\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\\\n    .getOrCreate()\n\nprint(\"‚úÖ Spark session with Iceberg support initialized!\")\nprint(f\"Spark version: {spark.version}\")\nprint(\"üßä Iceberg JARs loaded:\")\nfor jar in iceberg_jars:\n    print(f\"   - {os.path.basename(jar)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "create-iceberg-table",
   "metadata": {},
   "source": [
    "## 1. Create an Iceberg Table\n",
    "\n",
    "Let's create a sample Iceberg table with customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initial customer data\n",
    "customers_data = [\n",
    "    (1, \"Alice Johnson\", \"alice@email.com\", \"2023-01-15\", \"Premium\"),\n",
    "    (2, \"Bob Smith\", \"bob@email.com\", \"2023-02-20\", \"Standard\"),\n",
    "    (3, \"Carol Davis\", \"carol@email.com\", \"2023-03-10\", \"Premium\"),\n",
    "    (4, \"David Wilson\", \"david@email.com\", \"2023-04-05\", \"Standard\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"signup_date\", StringType(), False),\n",
    "    StructField(\"tier\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(customers_data, schema)\n",
    "\n",
    "# Create Iceberg table\n",
    "df.writeTo(\"iceberg.customers\").create()\n",
    "\n",
    "print(\"‚úÖ Created Iceberg table 'iceberg.customers'\")\n",
    "spark.sql(\"SELECT * FROM iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-travel-demo",
   "metadata": {},
   "source": [
    "## 2. Time Travel Queries\n",
    "\n",
    "Iceberg allows you to query data as it existed at any point in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current snapshot information\n",
    "snapshots = spark.sql(\"SELECT * FROM iceberg.customers.snapshots\")\n",
    "print(\"üì∏ Table snapshots:\")\n",
    "snapshots.select(\"snapshot_id\", \"timestamp_ms\", \"operation\").show()\n",
    "\n",
    "# Store first snapshot ID for time travel\n",
    "first_snapshot = snapshots.first()[\"snapshot_id\"]\n",
    "print(f\"First snapshot ID: {first_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more data to demonstrate time travel\n",
    "new_customers = [\n",
    "    (5, \"Eve Brown\", \"eve@email.com\", \"2024-01-15\", \"Premium\"),\n",
    "    (6, \"Frank Miller\", \"frank@email.com\", \"2024-02-20\", \"Standard\")\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_customers, schema)\n",
    "new_df.writeTo(\"iceberg.customers\").append()\n",
    "\n",
    "print(\"‚úÖ Added new customers\")\n",
    "print(\"Current data:\")\n",
    "spark.sql(\"SELECT COUNT(*) as current_count FROM iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-historical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query historical data using snapshot ID\n",
    "print(\"üï∞Ô∏è Time travel query - data at first snapshot:\")\n",
    "historical_query = f\"SELECT COUNT(*) as historical_count FROM iceberg.customers VERSION AS OF {first_snapshot}\"\n",
    "spark.sql(historical_query).show()\n",
    "\n",
    "print(\"Comparison:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    'Current' as timepoint, COUNT(*) as record_count \n",
    "FROM iceberg.customers\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Historical' as timepoint, COUNT(*) as record_count \n",
    "FROM iceberg.customers VERSION AS OF \"\"\" + str(first_snapshot) + \"\"\"\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-evolution",
   "metadata": {},
   "source": [
    "## 3. Schema Evolution\n",
    "\n",
    "Iceberg supports schema evolution without breaking existing queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evolve-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the table\n",
    "spark.sql(\"ALTER TABLE iceberg.customers ADD COLUMN phone STRING\")\n",
    "\n",
    "print(\"‚úÖ Added 'phone' column to table\")\n",
    "print(\"Updated schema:\")\n",
    "spark.sql(\"DESCRIBE iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert-evolved-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data with the new column\n",
    "evolved_customers = [\n",
    "    (7, \"Grace Lee\", \"grace@email.com\", \"2024-03-15\", \"Premium\", \"+1-555-0123\")\n",
    "]\n",
    "\n",
    "evolved_schema = schema.add(StructField(\"phone\", StringType(), True))\n",
    "evolved_df = spark.createDataFrame(evolved_customers, evolved_schema)\n",
    "evolved_df.writeTo(\"iceberg.customers\").append()\n",
    "\n",
    "print(\"‚úÖ Inserted data with new schema\")\n",
    "spark.sql(\"SELECT * FROM iceberg.customers WHERE phone IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-maintenance",
   "metadata": {},
   "source": [
    "## 4. Table Maintenance\n",
    "\n",
    "Iceberg provides operations for managing table snapshots and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table history\n",
    "print(\"üìã Table history:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.customers.history\").show()\n",
    "\n",
    "print(\"üìä Current snapshots:\")\n",
    "spark.sql(\"SELECT snapshot_id, timestamp_ms, operation, summary FROM iceberg.customers.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table files\n",
    "print(\"üìÅ Table files:\")\n",
    "files_df = spark.sql(\"SELECT file_path, file_format, record_count FROM iceberg.customers.files\")\n",
    "files_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollback-demo",
   "metadata": {},
   "source": [
    "## 5. Rollback Capability\n",
    "\n",
    "Iceberg allows you to rollback to previous snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current count\n",
    "print(\"Before rollback:\")\n",
    "spark.sql(\"SELECT COUNT(*) as count FROM iceberg.customers\").show()\n",
    "\n",
    "# Rollback to first snapshot\n",
    "rollback_sql = f\"CALL iceberg.system.rollback_to_snapshot('iceberg.customers', {first_snapshot})\"\n",
    "spark.sql(rollback_sql)\n",
    "\n",
    "print(\"‚úÖ Rolled back to first snapshot\")\n",
    "print(\"After rollback:\")\n",
    "spark.sql(\"SELECT COUNT(*) as count FROM iceberg.customers\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "This notebook demonstrated key Apache Iceberg features:\n",
    "\n",
    "‚úÖ **ACID Transactions** - All operations are atomic and consistent\n",
    "\n",
    "‚úÖ **Time Travel** - Query data as it existed at any point in time\n",
    "\n",
    "‚úÖ **Schema Evolution** - Add columns without breaking existing queries\n",
    "\n",
    "‚úÖ **Snapshot Management** - View and manage table versions\n",
    "\n",
    "‚úÖ **Rollback Capability** - Easily revert to previous states\n",
    "\n",
    "### Next Steps:\n",
    "- Explore partition evolution with `ALTER TABLE ... REPLACE PARTITION FIELD`\n",
    "- Set up branch and tag management for complex workflows\n",
    "- Integrate with Spark streaming for real-time Iceberg updates\n",
    "- Use Iceberg tables in your production analytics pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# spark.sql(\"DROP TABLE iceberg.customers\")\n",
    "# print(\"üßπ Cleaned up demo table\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}