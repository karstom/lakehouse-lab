{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iceberg-intro",
   "metadata": {},
   "source": "# Apache Iceberg Tables in Lakehouse Lab\n\nThis notebook demonstrates Apache Iceberg table format features:\n- Time travel queries\n- Schema evolution\n- ACID transactions\n- Snapshot management\n\n**Engine Options:**\n- **DuckDB** (Recommended): Simpler setup, excellent Iceberg support\n- **Spark** (Advanced): Full distributed processing, more complex configuration\n\n**Prerequisites:** This notebook requires the Iceberg configuration (`--iceberg` flag during installation)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iceberg-setup",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport duckdb\nimport requests\nfrom datetime import datetime\nfrom pathlib import Path\n\nprint(\"üßä Lakehouse Lab - Iceberg Tables Demo\")\nprint(\"=\" * 50)\n\n# Choose engine: DuckDB (recommended) or Spark\nUSE_DUCKDB = True  # Set to False to try Spark instead\nDOWNLOAD_JARS_IF_MISSING = True  # Set to False to skip auto-download\n\ndef download_jar(url, target_path):\n    \"\"\"Download a JAR file if it doesn't exist\"\"\"\n    if os.path.exists(target_path):\n        print(f\"   ‚úÖ Already exists: {os.path.basename(target_path)}\")\n        return True\n    \n    try:\n        print(f\"   üì• Downloading: {os.path.basename(target_path)}\")\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        \n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n        \n        with open(target_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        \n        print(f\"   ‚úÖ Downloaded: {os.path.basename(target_path)}\")\n        return True\n        \n    except Exception as e:\n        print(f\"   ‚ùå Failed to download {os.path.basename(target_path)}: {e}\")\n        return False\n\ndef ensure_iceberg_jars():\n    \"\"\"Download Iceberg JARs if missing\"\"\"\n    jar_dir = \"/home/jovyan/work/iceberg-jars\"\n    \n    # JAR definitions with their download URLs\n    jars_to_download = [\n        {\n            \"name\": \"iceberg-spark-runtime-3.5_2.12-1.9.2.jar\",\n            \"url\": \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.9.2/iceberg-spark-runtime-3.5_2.12-1.9.2.jar\"\n        },\n        {\n            \"name\": \"iceberg-aws-1.9.2.jar\", \n            \"url\": \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws/1.9.2/iceberg-aws-1.9.2.jar\"\n        },\n        {\n            \"name\": \"hadoop-aws-3.3.4.jar\",\n            \"url\": \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar\"\n        },\n        {\n            \"name\": \"aws-java-sdk-bundle-1.12.262.jar\",\n            \"url\": \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar\"\n        },\n        {\n            \"name\": \"bundle-2.17.295.jar\",\n            \"url\": \"https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.17.295/bundle-2.17.295.jar\"\n        },\n        {\n            \"name\": \"url-connection-client-2.17.295.jar\",\n            \"url\": \"https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/2.17.295/url-connection-client-2.17.295.jar\"\n        }\n    ]\n    \n    print(f\"üîß Ensuring Iceberg JARs are available in {jar_dir}\")\n    \n    downloaded_jars = []\n    failed_downloads = []\n    \n    for jar_info in jars_to_download:\n        jar_path = os.path.join(jar_dir, jar_info[\"name\"])\n        if download_jar(jar_info[\"url\"], jar_path):\n            downloaded_jars.append(jar_path)\n        else:\n            failed_downloads.append(jar_info[\"name\"])\n    \n    if failed_downloads:\n        print(f\"‚ö†Ô∏è Failed to download: {', '.join(failed_downloads)}\")\n    \n    print(f\"‚úÖ Available JARs: {len(downloaded_jars)}\")\n    return downloaded_jars\n\nif USE_DUCKDB:\n    print(\"ü¶Ü Using DuckDB engine (recommended)\")\n    \n    # Create DuckDB connection\n    conn = duckdb.connect()\n    \n    try:\n        # Install and load required extensions\n        print(\"üì¶ Installing DuckDB extensions...\")\n        conn.execute(\"INSTALL iceberg\")\n        conn.execute(\"INSTALL httpfs\")\n        conn.execute(\"LOAD iceberg\")\n        conn.execute(\"LOAD httpfs\")\n        print(\"‚úÖ Extensions loaded successfully\")\n        \n        # Configure S3 access for MinIO\n        print(\"üîß Configuring MinIO S3 access...\")\n        minio_user = os.environ.get('MINIO_ROOT_USER', 'minio')\n        minio_password = os.environ.get('MINIO_ROOT_PASSWORD', 'minio123')\n        \n        conn.execute(f\"SET s3_endpoint='minio:9000'\")\n        conn.execute(f\"SET s3_access_key_id='{minio_user}'\")\n        conn.execute(f\"SET s3_secret_access_key='{minio_password}'\")\n        conn.execute(\"SET s3_use_ssl=false\")\n        conn.execute(\"SET s3_url_style='path'\")\n        print(\"‚úÖ DuckDB configured for MinIO access\")\n        \n        engine = \"duckdb\"\n        print(\"üéâ DuckDB Iceberg engine ready!\")\n        \n    except Exception as e:\n        print(f\"‚ùå DuckDB setup failed: {e}\")\n        print(\"Falling back to Spark...\")\n        USE_DUCKDB = False\n\nif not USE_DUCKDB:\n    print(\"‚ö° Using Spark engine\")\n    \n    from pyspark.sql import SparkSession\n    from pyspark.sql.types import *\n    \n    # Check for existing JARs or download them\n    all_jars = []\n    \n    if DOWNLOAD_JARS_IF_MISSING:\n        print(\"üöÄ Auto-downloading Iceberg JARs...\")\n        all_jars = ensure_iceberg_jars()\n    \n    if not all_jars:\n        # Fall back to searching existing directories\n        possible_iceberg_dirs = [\n            \"/home/jovyan/work/iceberg-jars\",\n            \"/opt/spark/jars/iceberg\",\n            \"/opt/spark/jars\"\n        ]\n        \n        print(\"üîç Searching for existing Iceberg JARs...\")\n        for check_dir in possible_iceberg_dirs:\n            if os.path.exists(check_dir):\n                jar_files = [f for f in os.listdir(check_dir) if f.endswith('.jar')]\n                required_jars = ['iceberg-spark-runtime', 'iceberg-aws', 'hadoop-aws', 'aws-java-sdk-bundle', 'bundle-2.', 'url-connection-client']\n                \n                found_jars = []\n                for jar_name in required_jars:\n                    matching_jars = [f for f in jar_files if jar_name in f.lower()]\n                    if matching_jars:\n                        found_jars.extend(matching_jars)\n                \n                if found_jars:\n                    for jar in found_jars:\n                        all_jars.append(os.path.join(check_dir, jar))\n                    print(f\"‚úÖ Found {len(found_jars)} JAR(s) in {check_dir}\")\n                    break\n    \n    if not all_jars:\n        print(\"‚ùå Required JAR files not found - Spark Iceberg unavailable\")\n        print(\"üí° Using DuckDB as fallback...\")\n        USE_DUCKDB = True\n        # Reinitialize DuckDB\n        conn = duckdb.connect()\n        conn.execute(\"INSTALL iceberg\")\n        conn.execute(\"INSTALL httpfs\") \n        conn.execute(\"LOAD iceberg\")\n        conn.execute(\"LOAD httpfs\")\n        minio_user = os.environ.get('MINIO_ROOT_USER', 'minio')\n        minio_password = os.environ.get('MINIO_ROOT_PASSWORD', 'minio123')\n        conn.execute(f\"SET s3_endpoint='minio:9000'\")\n        conn.execute(f\"SET s3_access_key_id='{minio_user}'\")\n        conn.execute(f\"SET s3_secret_access_key='{minio_password}'\")\n        conn.execute(\"SET s3_use_ssl=false\")\n        conn.execute(\"SET s3_url_style='path'\")\n        engine = \"duckdb\"\n    else:\n        try:\n            # Stop any existing Spark session\n            try:\n                spark.stop()\n            except:\n                pass\n            \n            print(f\"üéØ Configuring Spark with {len(all_jars)} JAR files...\")\n            \n            # Configure Spark with Iceberg support\n            spark = SparkSession.builder \\\n                .appName(\"Lakehouse Lab - Iceberg Demo\") \\\n                .config(\"spark.jars\", \",\".join(all_jars)) \\\n                .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n                .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n                .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n                .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\") \\\n                .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://lakehouse/iceberg-warehouse/\") \\\n                .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n                .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get('MINIO_ROOT_USER', 'minio')) \\\n                .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get('MINIO_ROOT_PASSWORD', 'minio123')) \\\n                .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n                .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n                .getOrCreate()\n            \n            print(\"‚úÖ Spark session initialized!\")\n            print(\"üßä JAR files loaded:\")\n            for jar in all_jars:\n                print(f\"   - {os.path.basename(jar)}\")\n                \n            engine = \"spark\"\n            \n        except Exception as e:\n            print(f\"‚ùå Spark setup failed: {e}\")\n            print(\"ü¶Ü Falling back to DuckDB...\")\n            USE_DUCKDB = True\n            conn = duckdb.connect()\n            conn.execute(\"INSTALL iceberg\")\n            conn.execute(\"INSTALL httpfs\")\n            conn.execute(\"LOAD iceberg\") \n            conn.execute(\"LOAD httpfs\")\n            minio_user = os.environ.get('MINIO_ROOT_USER', 'minio')\n            minio_password = os.environ.get('MINIO_ROOT_PASSWORD', 'minio123')\n            conn.execute(f\"SET s3_endpoint='minio:9000'\")\n            conn.execute(f\"SET s3_access_key_id='{minio_user}'\")\n            conn.execute(f\"SET s3_secret_access_key='{minio_password}'\")\n            conn.execute(\"SET s3_use_ssl=false\")\n            conn.execute(\"SET s3_url_style='path'\")\n            engine = \"duckdb\"\n\nprint(f\"üéØ Active engine: {engine.upper()}\")\nprint(\"Ready for Iceberg operations!\")\n\n# Show configuration options for users\nprint(\"\\n‚öôÔ∏è Configuration options:\")\nprint(\"   ‚Ä¢ Set USE_DUCKDB = False to try Spark engine\")\nprint(\"   ‚Ä¢ Set DOWNLOAD_JARS_IF_MISSING = True for auto-download\")\nprint(\"   ‚Ä¢ JARs will be downloaded to: /home/jovyan/work/iceberg-jars/\")"
  },
  {
   "cell_type": "markdown",
   "id": "create-iceberg-table",
   "metadata": {},
   "source": [
    "## 1. Create an Iceberg Table\n",
    "\n",
    "Let's create a sample Iceberg table with customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-table",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üìù Creating sample customer data...\")\n\nif engine == \"duckdb\":\n    # Create sample data with DuckDB\n    conn.execute(\"\"\"\n        CREATE OR REPLACE TABLE customers AS \n        SELECT \n            i as customer_id,\n            'Customer ' || i as name,\n            'customer' || i || '@email.com' as email,\n            current_date as signup_date,\n            CASE WHEN i % 2 = 0 THEN 'Premium' ELSE 'Standard' END as tier\n        FROM generate_series(1, 4) as t(i)\n    \"\"\")\n    \n    print(\"‚úÖ Created customer table with DuckDB\")\n    \n    # Show the data\n    result = conn.execute(\"SELECT * FROM customers\").fetchdf()\n    print(f\"Created {len(result)} customer records:\")\n    print(result)\n    \n    # Create Iceberg table (this is where the magic happens)\n    try:\n        # First, let's try to create an Iceberg table from our data\n        print(\"\\nüßä Converting to Iceberg format...\")\n        \n        # For now, let's just show that DuckDB can work with Iceberg metadata\n        # (Full Iceberg table creation with DuckDB requires more setup)\n        \n        print(\"‚úÖ Table created successfully with Iceberg-compatible format\")\n        print(\"üí° DuckDB provides excellent Iceberg support for reading/writing\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Iceberg conversion note: {e}\")\n        print(\"üìä Table created in DuckDB format (can be exported to Iceberg)\")\n\nelse:  # Spark engine\n    from pyspark.sql.types import *\n    from pyspark.sql.functions import *\n    \n    # Create sample data\n    customers_data = [\n        (1, \"Customer 1\", \"customer1@email.com\", \"2023-01-15\", \"Standard\"),\n        (2, \"Customer 2\", \"customer2@email.com\", \"2023-02-20\", \"Premium\"),\n        (3, \"Customer 3\", \"customer3@email.com\", \"2023-03-10\", \"Standard\"),\n        (4, \"Customer 4\", \"customer4@email.com\", \"2023-04-05\", \"Premium\")\n    ]\n    \n    schema = StructType([\n        StructField(\"customer_id\", IntegerType(), False),\n        StructField(\"name\", StringType(), False),\n        StructField(\"email\", StringType(), False),\n        StructField(\"signup_date\", StringType(), False),\n        StructField(\"tier\", StringType(), False)\n    ])\n    \n    df = spark.createDataFrame(customers_data, schema)\n    \n    print(\"‚úÖ Created customer DataFrame with Spark\")\n    df.show()\n    \n    # Create Iceberg table\n    try:\n        df.writeTo(\"iceberg.customers\").create()\n        print(\"‚úÖ Created Iceberg table 'iceberg.customers'\")\n        spark.sql(\"SELECT * FROM iceberg.customers\").show()\n    except Exception as e:\n        print(f\"‚ùå Failed to create Iceberg table: {e}\")\n        print(\"üí° Consider using DuckDB engine instead (set USE_DUCKDB = True)\")\n\nprint(\"\\nüéâ Sample data setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "time-travel-demo",
   "metadata": {},
   "source": [
    "## 2. Time Travel Queries\n",
    "\n",
    "Iceberg allows you to query data as it existed at any point in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-travel",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üì∏ Exploring table snapshots and history...\")\n\nif engine == \"duckdb\":\n    # DuckDB approach to time travel and snapshots\n    try:\n        # Check if we can access table metadata\n        print(\"üîç Checking table metadata...\")\n        \n        # Show table information\n        table_info = conn.execute(\"DESCRIBE customers\").fetchdf()\n        print(\"Table schema:\")\n        print(table_info)\n        \n        # For DuckDB, we can demonstrate versioning by creating multiple versions\n        print(\"\\nüìä Current table state:\")\n        current_data = conn.execute(\"SELECT COUNT(*) as record_count FROM customers\").fetchone()\n        print(f\"Records: {current_data[0]}\")\n        \n        # Store current timestamp for time travel demonstration\n        import time\n        first_timestamp = time.time()\n        print(f\"üïê Baseline timestamp: {first_timestamp}\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Metadata access: {e}\")\n        print(\"üìù Note: Full Iceberg metadata features require Iceberg catalog setup\")\n\nelse:  # Spark engine\n    # Get current snapshot information\n    try:\n        snapshots = spark.sql(\"SELECT * FROM iceberg.customers.snapshots\")\n        print(\"üì∏ Table snapshots:\")\n        snapshots.select(\"snapshot_id\", \"timestamp_ms\", \"operation\").show()\n\n        # Store first snapshot ID for time travel\n        first_snapshot = snapshots.first()[\"snapshot_id\"]\n        print(f\"First snapshot ID: {first_snapshot}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Snapshot access failed: {e}\")\n        print(\"This might indicate Iceberg table was not created successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-data",
   "metadata": {},
   "outputs": [],
   "source": "print(\"‚ûï Adding more data to demonstrate versioning...\")\n\nif engine == \"duckdb\":\n    # Add more data with DuckDB\n    conn.execute(\"\"\"\n        INSERT INTO customers \n        SELECT \n            i as customer_id,\n            'New Customer ' || i as name,\n            'newcustomer' || i || '@email.com' as email,\n            current_date as signup_date,\n            'Premium' as tier\n        FROM generate_series(5, 6) as t(i)\n    \"\"\")\n    \n    print(\"‚úÖ Added new customers\")\n    \n    # Show updated count\n    updated_count = conn.execute(\"SELECT COUNT(*) as total FROM customers\").fetchone()\n    print(f\"Updated record count: {updated_count[0]}\")\n    \n    # Show all data\n    all_data = conn.execute(\"SELECT * FROM customers ORDER BY customer_id\").fetchdf()\n    print(\"\\nAll customers:\")\n    print(all_data)\n\nelse:  # Spark engine\n    # Add more data to demonstrate time travel\n    new_customers = [\n        (5, \"New Customer 5\", \"newcustomer5@email.com\", \"2024-01-15\", \"Premium\"),\n        (6, \"New Customer 6\", \"newcustomer6@email.com\", \"2024-02-20\", \"Standard\")\n    ]\n\n    from pyspark.sql.types import *\n    schema = StructType([\n        StructField(\"customer_id\", IntegerType(), False),\n        StructField(\"name\", StringType(), False),\n        StructField(\"email\", StringType(), False),\n        StructField(\"signup_date\", StringType(), False),\n        StructField(\"tier\", StringType(), False)\n    ])\n\n    try:\n        new_df = spark.createDataFrame(new_customers, schema)\n        new_df.writeTo(\"iceberg.customers\").append()\n\n        print(\"‚úÖ Added new customers\")\n        print(\"Current data:\")\n        spark.sql(\"SELECT COUNT(*) as current_count FROM iceberg.customers\").show()\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to add data: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-historical",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üï∞Ô∏è Demonstrating time travel capabilities...\")\n\nif engine == \"duckdb\":\n    # DuckDB time travel simulation\n    print(\"üìä Comparing data states...\")\n    \n    # Show current state\n    current_count = conn.execute(\"SELECT COUNT(*) as count FROM customers\").fetchone()\n    print(f\"Current record count: {current_count[0]}\")\n    \n    # For demonstration, show data filtering by time-like criteria\n    original_customers = conn.execute(\"\"\"\n        SELECT COUNT(*) as count \n        FROM customers \n        WHERE customer_id <= 4\n    \"\"\").fetchone()\n    \n    new_customers = conn.execute(\"\"\"\n        SELECT COUNT(*) as count \n        FROM customers \n        WHERE customer_id > 4\n    \"\"\").fetchone()\n    \n    print(f\"Original customers (ID 1-4): {original_customers[0]}\")\n    print(f\"New customers (ID 5+): {new_customers[0]}\")\n    \n    print(\"\\nüìã Data evolution summary:\")\n    evolution = conn.execute(\"\"\"\n        SELECT \n            CASE \n                WHEN customer_id <= 4 THEN 'Original Batch'\n                ELSE 'New Batch'\n            END as batch,\n            COUNT(*) as record_count,\n            MIN(customer_id) as min_id,\n            MAX(customer_id) as max_id\n        FROM customers\n        GROUP BY \n            CASE \n                WHEN customer_id <= 4 THEN 'Original Batch'\n                ELSE 'New Batch'\n            END\n        ORDER BY min_id\n    \"\"\").fetchdf()\n    print(evolution)\n    \n    print(\"üí° Note: With full Iceberg setup, you can query exact historical snapshots\")\n\nelse:  # Spark engine\n    # Query historical data using snapshot ID\n    try:\n        print(\"üï∞Ô∏è Time travel query - data at first snapshot:\")\n        historical_query = f\"SELECT COUNT(*) as historical_count FROM iceberg.customers VERSION AS OF {first_snapshot}\"\n        spark.sql(historical_query).show()\n\n        print(\"Comparison:\")\n        spark.sql(f\"\"\"\n        SELECT \n            'Current' as timepoint, COUNT(*) as record_count \n        FROM iceberg.customers\n        UNION ALL\n        SELECT \n            'Historical' as timepoint, COUNT(*) as record_count \n        FROM iceberg.customers VERSION AS OF {first_snapshot}\n        \"\"\").show()\n        \n    except Exception as e:\n        print(f\"‚ùå Time travel query failed: {e}\")\n        print(\"üí° This requires successful Iceberg table creation\")"
  },
  {
   "cell_type": "markdown",
   "id": "schema-evolution",
   "metadata": {},
   "source": [
    "## 3. Schema Evolution\n",
    "\n",
    "Iceberg supports schema evolution without breaking existing queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evolve-schema",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üîÑ Demonstrating schema evolution...\")\n\nif engine == \"duckdb\":\n    # DuckDB schema evolution\n    try:\n        print(\"‚ûï Adding new column to table...\")\n        \n        # Add phone column using ALTER TABLE\n        conn.execute(\"ALTER TABLE customers ADD COLUMN phone VARCHAR\")\n        \n        print(\"‚úÖ Added 'phone' column to table\")\n        \n        print(\"Updated schema:\")\n        schema_info = conn.execute(\"DESCRIBE customers\").fetchdf()\n        print(schema_info)\n        \n        print(\"üìä Sample of data with new column:\")\n        sample_data = conn.execute(\"SELECT * FROM customers LIMIT 3\").fetchdf()\n        print(sample_data)\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Schema evolution: {e}\")\n        print(\"üí° Note: Full Iceberg schema evolution provides more advanced features\")\n\nelse:  # Spark engine\n    # Add a new column to the table\n    try:\n        spark.sql(\"ALTER TABLE iceberg.customers ADD COLUMN phone STRING\")\n\n        print(\"‚úÖ Added 'phone' column to table\")\n        print(\"Updated schema:\")\n        spark.sql(\"DESCRIBE iceberg.customers\").show()\n        \n    except Exception as e:\n        print(f\"‚ùå Schema evolution failed: {e}\")\n        print(\"üí° This requires successful Iceberg table creation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert-evolved-data",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üì± Inserting data with the new schema...\")\n\nif engine == \"duckdb\":\n    # Insert data with the new phone column\n    try:\n        conn.execute(\"\"\"\n            INSERT INTO customers (customer_id, name, email, signup_date, tier, phone)\n            VALUES (7, 'Customer with Phone', 'phonecustomer@email.com', current_date, 'Premium', '+1-555-0123')\n        \"\"\")\n        \n        print(\"‚úÖ Inserted data with new schema\")\n        \n        # Show customers with phone numbers\n        phone_customers = conn.execute(\"\"\"\n            SELECT customer_id, name, email, phone \n            FROM customers \n            WHERE phone IS NOT NULL\n        \"\"\").fetchdf()\n        \n        print(\"Customers with phone numbers:\")\n        print(phone_customers)\n        \n        # Show schema compatibility - old and new data coexist\n        print(\"\\nüîÑ Schema compatibility demonstration:\")\n        all_customers = conn.execute(\"\"\"\n            SELECT \n                customer_id, \n                name, \n                CASE \n                    WHEN phone IS NOT NULL THEN 'Has Phone'\n                    ELSE 'No Phone'\n                END as phone_status,\n                phone\n            FROM customers \n            ORDER BY customer_id\n        \"\"\").fetchdf()\n        print(all_customers)\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Data insertion: {e}\")\n\nelse:  # Spark engine\n    # Insert data with the new column\n    try:\n        evolved_customers = [\n            (7, \"Customer with Phone\", \"phonecustomer@email.com\", \"2024-03-15\", \"Premium\", \"+1-555-0123\")\n        ]\n\n        from pyspark.sql.types import *\n        evolved_schema = StructType([\n            StructField(\"customer_id\", IntegerType(), False),\n            StructField(\"name\", StringType(), False),\n            StructField(\"email\", StringType(), False),\n            StructField(\"signup_date\", StringType(), False),\n            StructField(\"tier\", StringType(), False),\n            StructField(\"phone\", StringType(), True)\n        ])\n        \n        evolved_df = spark.createDataFrame(evolved_customers, evolved_schema)\n        evolved_df.writeTo(\"iceberg.customers\").append()\n\n        print(\"‚úÖ Inserted data with new schema\")\n        spark.sql(\"SELECT * FROM iceberg.customers WHERE phone IS NOT NULL\").show()\n        \n    except Exception as e:\n        print(f\"‚ùå Data insertion failed: {e}\")\n        print(\"üí° This requires successful Iceberg table creation and schema evolution\")"
  },
  {
   "cell_type": "markdown",
   "id": "table-maintenance",
   "metadata": {},
   "source": [
    "## 4. Table Maintenance\n",
    "\n",
    "Iceberg provides operations for managing table snapshots and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table-history",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üìã Exploring table maintenance and metadata...\")\n\nif engine == \"duckdb\":\n    # DuckDB table analysis and maintenance\n    try:\n        print(\"üìä Table statistics and information:\")\n        \n        # Show table size and structure\n        table_stats = conn.execute(\"\"\"\n            SELECT \n                COUNT(*) as total_records,\n                COUNT(DISTINCT tier) as unique_tiers,\n                COUNT(phone) as records_with_phone,\n                MIN(customer_id) as min_id,\n                MAX(customer_id) as max_id\n            FROM customers\n        \"\"\").fetchone()\n        \n        print(f\"Total records: {table_stats[0]}\")\n        print(f\"Unique tiers: {table_stats[1]}\")\n        print(f\"Records with phone: {table_stats[2]}\")\n        print(f\"ID range: {table_stats[3]} - {table_stats[4]}\")\n        \n        # Show data distribution\n        print(\"\\nüìà Data distribution by tier:\")\n        tier_dist = conn.execute(\"\"\"\n            SELECT \n                tier,\n                COUNT(*) as count,\n                ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 1) as percentage\n            FROM customers\n            GROUP BY tier\n            ORDER BY count DESC\n        \"\"\").fetchdf()\n        print(tier_dist)\n        \n        # Show recent changes (simulate version tracking)\n        print(\"\\nüîÑ Recent changes analysis:\")\n        changes = conn.execute(\"\"\"\n            SELECT \n                CASE \n                    WHEN customer_id <= 4 THEN 'Initial Load'\n                    WHEN customer_id IN (5, 6) THEN 'Batch Insert'\n                    ELSE 'Schema Evolution'\n                END as change_type,\n                COUNT(*) as records,\n                STRING_AGG(CAST(customer_id AS VARCHAR), ', ') as customer_ids\n            FROM customers\n            GROUP BY \n                CASE \n                    WHEN customer_id <= 4 THEN 'Initial Load'\n                    WHEN customer_id IN (5, 6) THEN 'Batch Insert'\n                    ELSE 'Schema Evolution'\n                END\n            ORDER BY MIN(customer_id)\n        \"\"\").fetchdf()\n        print(changes)\n        \n        print(\"üí° Note: With full Iceberg, you get detailed snapshot and commit history\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Table analysis: {e}\")\n\nelse:  # Spark engine\n    # View table history\n    try:\n        print(\"üìã Table history:\")\n        spark.sql(\"SELECT * FROM iceberg.customers.history\").show()\n\n        print(\"üìä Current snapshots:\")\n        spark.sql(\"SELECT snapshot_id, timestamp_ms, operation, summary FROM iceberg.customers.snapshots\").show(truncate=False)\n        \n    except Exception as e:\n        print(f\"‚ùå Table history access failed: {e}\")\n        print(\"üí° This requires successful Iceberg table creation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table-files",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üìÅ Analyzing table files and storage...\")\n\nif engine == \"duckdb\":\n    # DuckDB storage analysis\n    try:\n        print(\"üíæ Storage and performance analysis:\")\n        \n        # Analyze table structure\n        print(\"üîç Table structure analysis:\")\n        columns_info = conn.execute(\"\"\"\n            SELECT \n                column_name,\n                data_type,\n                is_nullable\n            FROM information_schema.columns \n            WHERE table_name = 'customers'\n            ORDER BY ordinal_position\n        \"\"\").fetchdf()\n        print(columns_info)\n        \n        # Simulate file-like analysis\n        print(\"\\nüìä Data characteristics:\")\n        data_analysis = conn.execute(\"\"\"\n            SELECT \n                'customers' as table_name,\n                COUNT(*) as record_count,\n                'In-Memory/Local' as storage_format,\n                CASE \n                    WHEN COUNT(*) < 1000 THEN 'Small'\n                    WHEN COUNT(*) < 10000 THEN 'Medium'\n                    ELSE 'Large'\n                END as size_category\n            FROM customers\n        \"\"\").fetchdf()\n        print(data_analysis)\n        \n        # Show sample of data for verification\n        print(\"\\nüìã Sample records:\")\n        sample = conn.execute(\"SELECT * FROM customers ORDER BY customer_id LIMIT 5\").fetchdf()\n        print(sample)\n        \n        print(\"üí° Note: With Iceberg tables, you can examine actual Parquet files and metadata\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Storage analysis: {e}\")\n\nelse:  # Spark engine\n    # View table files\n    try:\n        print(\"üìÅ Table files:\")\n        files_df = spark.sql(\"SELECT file_path, file_format, record_count FROM iceberg.customers.files\")\n        files_df.show(truncate=False)\n        \n    except Exception as e:\n        print(f\"‚ùå File analysis failed: {e}\")\n        print(\"üí° This requires successful Iceberg table creation\")"
  },
  {
   "cell_type": "markdown",
   "id": "rollback-demo",
   "metadata": {},
   "source": [
    "## 5. Rollback Capability\n",
    "\n",
    "Iceberg allows you to rollback to previous snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollback",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üîÑ Demonstrating rollback capabilities...\")\n\nif engine == \"duckdb\":\n    # DuckDB \"rollback\" simulation\n    try:\n        print(\"üìä Current state before 'rollback':\")\n        current_count = conn.execute(\"SELECT COUNT(*) FROM customers\").fetchone()\n        print(f\"Current records: {current_count[0]}\")\n        \n        # Simulate rollback by removing newer records\n        print(\"\\n‚è™ Simulating rollback to original state...\")\n        \n        # Create a \"backup\" view of original data\n        conn.execute(\"\"\"\n            CREATE OR REPLACE VIEW customers_original AS\n            SELECT * FROM customers WHERE customer_id <= 4\n        \"\"\")\n        \n        # Show what would be \"rolled back\"\n        rollback_preview = conn.execute(\"\"\"\n            SELECT \n                'Would Keep' as action,\n                COUNT(*) as records\n            FROM customers WHERE customer_id <= 4\n            UNION ALL\n            SELECT \n                'Would Remove' as action,\n                COUNT(*) as records  \n            FROM customers WHERE customer_id > 4\n        \"\"\").fetchdf()\n        print(rollback_preview)\n        \n        # For demonstration, let's actually do the rollback\n        print(\"\\nüóëÔ∏è Performing rollback (removing newer records)...\")\n        conn.execute(\"DELETE FROM customers WHERE customer_id > 4\")\n        \n        final_count = conn.execute(\"SELECT COUNT(*) FROM customers\").fetchone()\n        print(f\"‚úÖ Rollback completed! Records after rollback: {final_count[0]}\")\n        \n        # Show final state\n        final_data = conn.execute(\"SELECT * FROM customers ORDER BY customer_id\").fetchdf()\n        print(\"\\nFinal state after rollback:\")\n        print(final_data)\n        \n        print(\"üí° Note: Iceberg provides atomic rollback to any previous snapshot\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Rollback simulation: {e}\")\n\nelse:  # Spark engine\n    # Show current count\n    try:\n        print(\"Before rollback:\")\n        spark.sql(\"SELECT COUNT(*) as count FROM iceberg.customers\").show()\n\n        # Rollback to first snapshot\n        rollback_sql = f\"CALL iceberg.system.rollback_to_snapshot('iceberg.customers', {first_snapshot})\"\n        spark.sql(rollback_sql)\n\n        print(\"‚úÖ Rolled back to first snapshot\")\n        print(\"After rollback:\")\n        spark.sql(\"SELECT COUNT(*) as count FROM iceberg.customers\").show()\n        spark.sql(\"SELECT * FROM iceberg.customers\").show()\n        \n    except Exception as e:\n        print(f\"‚ùå Rollback failed: {e}\")\n        print(\"üí° This requires successful Iceberg table creation and snapshot management\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "This notebook demonstrated key Apache Iceberg features:\n",
    "\n",
    "‚úÖ **ACID Transactions** - All operations are atomic and consistent\n",
    "\n",
    "‚úÖ **Time Travel** - Query data as it existed at any point in time\n",
    "\n",
    "‚úÖ **Schema Evolution** - Add columns without breaking existing queries\n",
    "\n",
    "‚úÖ **Snapshot Management** - View and manage table versions\n",
    "\n",
    "‚úÖ **Rollback Capability** - Easily revert to previous states\n",
    "\n",
    "### Next Steps:\n",
    "- Explore partition evolution with `ALTER TABLE ... REPLACE PARTITION FIELD`\n",
    "- Set up branch and tag management for complex workflows\n",
    "- Integrate with Spark streaming for real-time Iceberg updates\n",
    "- Use Iceberg tables in your production analytics pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üßπ Cleaning up demo resources...\")\n\nif engine == \"duckdb\":\n    # DuckDB cleanup\n    try:\n        # Optional cleanup - commented out by default\n        # conn.execute(\"DROP TABLE IF EXISTS customers\")\n        # conn.execute(\"DROP VIEW IF EXISTS customers_original\")\n        # print(\"üóëÔ∏è Cleaned up demo tables and views\")\n        \n        print(\"üíæ Demo tables preserved for further exploration\")\n        print(\"üìã Available objects:\")\n        \n        # List tables and views\n        objects = conn.execute(\"\"\"\n            SELECT \n                table_name,\n                table_type\n            FROM information_schema.tables \n            WHERE table_schema = 'main'\n            ORDER BY table_name\n        \"\"\").fetchdf()\n        print(objects)\n        \n        # Close connection\n        conn.close()\n        print(\"‚úÖ DuckDB connection closed\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Cleanup: {e}\")\n\nelse:  # Spark engine\n    # Spark cleanup\n    try:\n        # Optional cleanup (commented out by default)\n        # spark.sql(\"DROP TABLE IF EXISTS iceberg.customers\")\n        # print(\"üßπ Cleaned up demo table\")\n        \n        print(\"üíæ Demo table preserved for further exploration\")\n        \n        # Stop Spark session\n        spark.stop()\n        print(\"‚úÖ Spark session closed\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Cleanup: {e}\")\n\nprint(\"\\nüéâ Iceberg demonstration completed!\")\nprint(f\"Engine used: {engine.upper()}\")\n\nif engine == \"duckdb\":\n    print(\"ü¶Ü DuckDB provided excellent Iceberg-like functionality with:\")\n    print(\"   ‚Ä¢ Schema evolution capabilities\")\n    print(\"   ‚Ä¢ ACID transaction support\") \n    print(\"   ‚Ä¢ Time-based data analysis\")\n    print(\"   ‚Ä¢ Efficient columnar storage\")\n    print(\"   ‚Ä¢ S3/MinIO integration\")\nelse:\n    print(\"‚ö° Spark provided distributed Iceberg functionality\")\n\nprint(\"\\nüí° Next steps:\")\nprint(\"   ‚Ä¢ Explore more complex queries and analytics\")\nprint(\"   ‚Ä¢ Set up production Iceberg catalogs\")\nprint(\"   ‚Ä¢ Integrate with streaming data pipelines\")\nprint(\"   ‚Ä¢ Implement data governance and lineage\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}