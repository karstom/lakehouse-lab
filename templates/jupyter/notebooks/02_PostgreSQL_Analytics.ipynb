   "metadata": {},
   "outputs": [],
   "source": [
    "# Query sample data\n",
    "result = conn.execute(\"\"\"\n",
    "    SELECT * FROM read_csv_auto('s3://lakehouse/raw-data/sample_orders.csv')\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Sample data from MinIO:\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics with DuckDB 1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analytics query\n",
    "analytics = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        product_category,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as total_revenue,\n",
    "        AVG(total_amount) as avg_order_value\n",
    "    FROM read_csv_auto('s3://lakehouse/raw-data/sample_orders.csv')\n",
    "    GROUP BY product_category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Sales by Product Category:\")\n",
    "display(analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lakehouse Lab\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"✅ Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Explore Superset**: Open http://localhost:9030 to create dashboards\n",
    "2. **Check Airflow**: Visit http://localhost:9020 to see workflow orchestration\n",
    "3. **Monitor with Portainer**: Use http://localhost:9060 for container management\n",
    "4. **Access MinIO Console**: Visit http://localhost:9001 for file management\n",
    "\n",
    "## Issues Fixed\n",
    "\n",
    "✅ **Issue #1**: Superset S3 configuration now persistent  \n",
    "✅ **Issue #2**: Airflow DuckDB import errors resolved  \n",
