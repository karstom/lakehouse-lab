#!/bin/bash

# Lakehouse Lab Startup Script with Error Handling
# Compatible with current Docker Compose configuration

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
LAKEHOUSE_ROOT="${LAKEHOUSE_ROOT:-./lakehouse-data}"
STARTUP_MODE="${1:-normal}"  # normal, debug, minimal
MAX_RETRIES=2
RETRY_COUNT=0

# Logging functions
log_info() {
    echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"
}

log_success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

log_warning() {
    echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"
}

log_error() {
    echo -e "${RED}‚ùå $1${NC}"
}

# Function to detect host IP address (runs on HOST, not in container)
detect_host_ip() {
    local detected_ip
    
    # Prefer HOST_IP environment variable if set and valid
    if [[ -n "${HOST_IP:-}" && "$HOST_IP" != "localhost" && "$HOST_IP" != "127.0.0.1" ]]; then
        echo "$HOST_IP"
        return 0
    fi
    
    # Try to detect from the host system - multiple fallback methods
    # Method 1: Use hostname -I (most reliable on Linux)
    if command -v hostname >/dev/null 2>&1; then
        detected_ip=$(hostname -I 2>/dev/null | awk '{print $1}')
        if [[ -n "$detected_ip" && "$detected_ip" != "127.0.0.1" && ! "$detected_ip" =~ ^172\.(1[6-9]|2[0-9]|3[0-1])\. ]]; then
            echo "$detected_ip"
            return 0
        fi
    fi
    
    # Method 2: Use ip route to get the IP used for external connectivity
    detected_ip=$(ip route get 8.8.8.8 2>/dev/null | awk '/src/ {print $7}' | head -1)
    if [[ -n "$detected_ip" && "$detected_ip" != "127.0.0.1" && ! "$detected_ip" =~ ^172\.(1[6-9]|2[0-9]|3[0-1])\. ]]; then
        echo "$detected_ip"
        return 0
    fi
    
    # Method 3: Get first non-loopback, non-Docker IP from interfaces
    detected_ip=$(ip addr show | grep 'inet ' | grep -v '127.0.0.1' | grep -v 'docker' | grep -v '172\.1[6-9]\.' | grep -v '172\.2[0-9]\.' | grep -v '172\.3[0-1]\.' | head -1 | awk '{print $2}' | cut -d'/' -f1)
    if [[ -n "$detected_ip" && "$detected_ip" != "127.0.0.1" ]]; then
        echo "$detected_ip"
        return 0
    fi
    
    # Method 4: Try using default gateway interface (exclude Docker interfaces)
    local default_interface=$(ip route | grep '^default' | awk '{print $5}' | head -1)
    if [[ -n "$default_interface" && "$default_interface" != docker* && "$default_interface" != br-* ]]; then
        detected_ip=$(ip addr show "$default_interface" 2>/dev/null | grep 'inet ' | awk '{print $2}' | cut -d'/' -f1)
        if [[ -n "$detected_ip" && "$detected_ip" != "127.0.0.1" ]]; then
            echo "$detected_ip"
            return 0
        fi
    fi
    
    # Final fallback to localhost
    echo "localhost"
}

echo -e "${BLUE}üè† Lakehouse Lab Startup Script${NC}"
echo -e "${BLUE}================================${NC}"
echo ""

# Check Docker
if ! command -v docker &> /dev/null; then
    echo -e "${RED}‚ùå Docker not found. Please install Docker first.${NC}"
    exit 1
fi

if ! docker compose version &> /dev/null; then
    echo -e "${RED}‚ùå Docker Compose not found. Please install Docker Compose first.${NC}"
    exit 1
fi

# Detect and set HOST_IP for service URLs
echo -e "${YELLOW}üåê Detecting host IP address...${NC}"
DETECTED_HOST_IP=$(detect_host_ip)

# Export HOST_IP for Docker Compose and containers to use
export HOST_IP="$DETECTED_HOST_IP"

if [[ "$HOST_IP" == "localhost" ]]; then
    echo -e "${YELLOW}   Using localhost - services accessible on this machine only${NC}"
    echo -e "${BLUE}   Tip: Set HOST_IP environment variable for remote access${NC}"
else
    echo -e "${GREEN}   Using detected IP: $HOST_IP${NC}"
    echo -e "${BLUE}   Services will be accessible remotely at this IP${NC}"
    
    # Check if Homepage config needs updating (contains Docker IPs)
    homepage_config="${LAKEHOUSE_ROOT:-./lakehouse-data}/homepage/config/services.yaml"
    if [[ -f "$homepage_config" ]] && grep -q "172\.[0-9]\+\.[0-9]\+\.[0-9]\+" "$homepage_config" 2>/dev/null; then
        echo -e "${YELLOW}   üìã Note: Homepage dashboard will be updated with new IP addresses${NC}"
    fi
fi
echo ""

# Function to check service health
check_service_health() {
    local service_name=$1
    local health_url=$2
    local max_attempts=${3:-20}
    
    echo -e "${YELLOW}‚è≥ Checking $service_name health...${NC}"
    
    for i in $(seq 1 $max_attempts); do
        if curl -sf "$health_url" >/dev/null 2>&1; then
            echo -e "${GREEN}‚úÖ $service_name is healthy${NC}"
            return 0
        fi
        echo -e "   Attempt $i/$max_attempts - waiting 5s..."
        sleep 5
    done
    
    echo -e "${RED}‚ùå $service_name failed to become healthy${NC}"
    return 1
}

# Function to check if curl is available, install if needed
ensure_curl() {
    if ! command -v curl &> /dev/null; then
        echo -e "${YELLOW}‚ö†Ô∏è  curl not found, trying to install...${NC}"
        # Try common package managers
        if command -v apt-get &> /dev/null; then
            sudo apt-get update && sudo apt-get install -y curl
        elif command -v yum &> /dev/null; then
            sudo yum install -y curl
        elif command -v brew &> /dev/null; then
            brew install curl
        else
            echo -e "${RED}‚ùå Cannot install curl automatically. Please install curl manually.${NC}"
            exit 1
        fi
    fi
}

# Function to create required named volumes if they don't exist
create_named_volumes() {
    log_info "Ensuring required named volumes exist..."
    
    # Get the project name (directory name)
    local project_name=$(basename "$(pwd)" | tr '[:upper:]' '[:lower:]')
    
    # List of all required volumes for the lakehouse stack
    local volumes=(
        "postgres_data"
        "minio_data" 
        "lakehouse_shared"
        "jupyter_notebooks"
        "jupyter_config"
        "airflow_dags"
        "airflow_logs"
        "airflow_plugins"
        "spark_jobs"
        "spark_logs"
        "superset_data"
        "homepage_config"
        "homepage_images"
        "vizro_data"
        "lancedb_data"
        "portainer_data"
    )
    
    # Create volumes if they don't exist (silently ignore if they already exist)
    for volume in "${volumes[@]}"; do
        docker volume create "${project_name}_${volume}" >/dev/null 2>&1 || true
        
        # Fix ownership for Airflow volumes if they need special permissions
        if [[ "$volume" == "airflow_"* ]]; then
            # Use a temporary container to fix Airflow volume ownership
            docker run --rm \
                -v "${project_name}_${volume}:/mnt/volume" \
                --user root \
                alpine:latest \
                sh -c "chown -R ${AIRFLOW_UID:-50000}:0 /mnt/volume && chmod -R 755 /mnt/volume" >/dev/null 2>&1 || true
        fi
    done
    
    # Create overlay-specific volumes if overlays are detected
    if [[ "${ENABLE_ICEBERG_OVERRIDE:-}" == "true" ]] || [[ -f ".iceberg-enabled" ]]; then
        docker volume create "${project_name}_iceberg_jars" >/dev/null 2>&1 || true
        docker volume create "${project_name}_iceberg_warehouse" >/dev/null 2>&1 || true
    fi
    
    if [[ -f "docker-compose.jupyterhub.yml" ]]; then
        docker volume create "${project_name}_jupyterhub_users" >/dev/null 2>&1 || true
        docker volume create "${project_name}_jupyterhub_shared" >/dev/null 2>&1 || true
    fi
    
    if [[ -f "docker-compose.auth.yml" ]]; then
        docker volume create "${project_name}_auth_data" >/dev/null 2>&1 || true
        docker volume create "${project_name}_audit_logs" >/dev/null 2>&1 || true
    fi
}

# Function to start services with dependency checking
start_with_dependencies() {
    echo -e "${BLUE}üöÄ Starting Lakehouse Lab ($STARTUP_MODE mode, attempt $((RETRY_COUNT + 1))/$((MAX_RETRIES + 1)))...${NC}"
    
    # Ensure curl is available for health checks
    ensure_curl
    
    # Ensure data directory exists
    mkdir -p "$LAKEHOUSE_ROOT"
    
    # Ensure all required named volumes exist (for external volume declarations)
    create_named_volumes
    
    case "$STARTUP_MODE" in
        "minimal")
            echo -e "${YELLOW}üì¶ Starting minimal services (storage + basic query)...${NC}"
            docker compose up -d postgres minio
            check_service_health "MinIO" "http://localhost:9000/minio/health/live"
            
            # Start lakehouse-init to set up buckets and sample data
            echo -e "${BLUE}üîß Running initialization...${NC}"
            docker compose up lakehouse-init
            
            # Start Homepage for service links
            docker compose up -d homepage
            ;;
            
        "debug")
            echo -e "${YELLOW}üîç Starting in debug mode (services one by one)...${NC}"
            
            # Check if Iceberg support should be enabled
            local compose_files="-f docker-compose.yml"
            if [[ "${ENABLE_ICEBERG_OVERRIDE:-}" == "true" ]] || [[ -f ".iceberg-enabled" ]]; then
                echo -e "${BLUE}üßä Iceberg support detected - using enhanced configuration...${NC}"
                compose_files="-f docker-compose.yml -f docker-compose.iceberg.yml"
                # Create marker file for future starts
                touch .iceberg-enabled
            fi
            
            # Layer 1: Storage
            echo -e "${BLUE}Layer 1: Storage services${NC}"
            docker compose $compose_files up -d postgres minio
            check_service_health "MinIO" "http://localhost:9000/minio/health/live"
            
            # Layer 2: Processing
            echo -e "${BLUE}Layer 2: Compute engines${NC}"
            docker compose $compose_files up -d spark-master spark-worker
            check_service_health "Spark Master" "http://localhost:8080" 15
            
            # Layer 3: Initialization
            echo -e "${BLUE}Layer 3: Data initialization${NC}"
            docker compose $compose_files up lakehouse-init
            
            # Layer 4: Applications
            echo -e "${BLUE}Layer 4: User applications${NC}"
            docker compose $compose_files up -d jupyter airflow-init
            sleep 30  # Give airflow-init time to complete
            docker compose $compose_files up -d airflow-scheduler airflow-webserver
            
            # Layer 5: BI and monitoring
            echo -e "${BLUE}Layer 5: BI and monitoring${NC}"
            docker compose $compose_files up -d superset portainer
            
            # Optional services
            echo -e "${BLUE}Layer 6: Optional services${NC}"
            docker compose --profile optional up -d homepage || echo -e "${YELLOW}‚ö†Ô∏è  Homepage is optional and may not start${NC}"
            ;;
            
        "normal"|*)
            echo -e "${YELLOW}üì¶ Starting all services...${NC}"
            
            # Check for service configuration override
            local compose_files="-f docker-compose.yml"
            if [[ -f "docker-compose.override.yml" ]]; then
                log_info "Using service configuration override"
                compose_files="-f docker-compose.yml -f docker-compose.override.yml"
            fi
            
            # Check if Iceberg support should be enabled
            if [[ "${ENABLE_ICEBERG_OVERRIDE:-}" == "true" ]] || [[ -f ".iceberg-enabled" ]]; then
                echo -e "${BLUE}üßä Iceberg support detected - using enhanced configuration...${NC}"
                compose_files="$compose_files -f docker-compose.iceberg.yml"
                # Try Iceberg startup first
                if docker compose $compose_files up -d; then
                    echo -e "${GREEN}‚úÖ All services with Iceberg support started successfully${NC}"
                    # Create marker file for future starts
                    touch .iceberg-enabled
                else
                    echo -e "${YELLOW}‚ö†Ô∏è  Iceberg startup failed, falling back to standard configuration...${NC}"
                    if docker compose -f docker-compose.yml up -d; then
                        echo -e "${GREEN}‚úÖ Standard services started successfully${NC}"
                    fi
                fi
            # Try normal startup first
            elif docker compose $compose_files up -d; then
                echo -e "${GREEN}‚úÖ All services started successfully${NC}"
                
                # Check key services
                sleep 10
                check_service_health "MinIO" "http://localhost:9000/minio/health/live" 10
                check_service_health "Spark Master" "http://localhost:8080" 15
                
            else
                echo -e "${RED}‚ùå Normal startup failed${NC}"
                
                # Check retry limits to prevent infinite loops
                if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                    RETRY_COUNT=$((RETRY_COUNT + 1))
                    echo -e "${YELLOW}‚è≥ Retrying with debug mode (attempt $((RETRY_COUNT + 1))/$((MAX_RETRIES + 1)))...${NC}"
                    sleep 5
                    
                    # Switch to debug mode for retry
                    STARTUP_MODE="debug"
                    start_with_dependencies
                    return
                else
                    echo -e "${RED}‚ùå Maximum retries ($MAX_RETRIES) exceeded. Startup failed.${NC}"
                    echo -e "${YELLOW}üí° Try running: ./start-lakehouse.sh debug${NC}"
                    echo -e "${YELLOW}üí° Or check logs: ./start-lakehouse.sh logs${NC}"
                    exit 1
                fi
            fi
            ;;
    esac
    
    # Final status check
    echo -e "${BLUE}üìä Final status check...${NC}"
    docker compose ps
    
    echo ""
    echo -e "${GREEN}üéâ Lakehouse Lab is ready!${NC}"
    echo ""
    echo -e "${BLUE}Access points:${NC}"
    echo -e "  üê≥ Portainer:         ${GREEN}http://${HOST_IP}:9060${NC} (container management)"
    echo -e "  üìà Superset BI:       ${GREEN}http://${HOST_IP}:9030${NC} (use ./scripts/show-credentials.sh for login)"
    echo -e "  üìã Airflow:           ${GREEN}http://${HOST_IP}:9020${NC} (use ./scripts/show-credentials.sh for login)"
    echo -e "  üìì JupyterLab:        ${GREEN}http://${HOST_IP}:9040${NC} (use ./scripts/show-credentials.sh for token)"
    echo -e "  üìä Vizro Dashboards:  ${GREEN}http://${HOST_IP}:9050${NC}"
    echo -e "  ü§ñ LanceDB API:       ${GREEN}http://${HOST_IP}:9080${NC} (docs: /docs)"
    echo -e "  ‚òÅÔ∏è  MinIO Console:     ${GREEN}http://${HOST_IP}:9001${NC} (use ./scripts/show-credentials.sh for login)"
    echo -e "  ‚ö° Spark Master:      ${GREEN}http://${HOST_IP}:8080${NC}"
    echo -e "  üè† Service Links:     ${GREEN}http://${HOST_IP}:9061${NC} (optional Homer)"
    
    # Show only enabled services if override exists
    if [[ -f "docker-compose.override.yml" ]]; then
        echo ""
        echo -e "${BLUE}üí° Note: Some services may be disabled via configuration${NC}"
        echo -e "    Use './scripts/configure-services.sh show' to see current settings"
    fi
    
    # Show Iceberg status if enabled
    if [[ -f ".iceberg-enabled" ]] || [[ "${ENABLE_ICEBERG_OVERRIDE:-}" == "true" ]]; then
        echo ""
        echo -e "${BLUE}üßä Iceberg Features Enabled:${NC}"
        echo -e "  ‚Ä¢ Time travel and versioning"
        echo -e "  ‚Ä¢ Schema evolution"  
        echo -e "  ‚Ä¢ ACID transactions"
        echo -e "  ‚Ä¢ Try the '03_Iceberg_Tables.ipynb' notebook!"
    fi
    
    echo ""
    echo -e "${YELLOW}üí° Tip: Use Portainer (${HOST_IP}:9060) for container management and monitoring${NC}"
    echo -e "${YELLOW}‚ö†Ô∏è  IMPORTANT: Set up Portainer admin account within 5 minutes or you'll be locked out!${NC}"
    echo -e "${YELLOW}üìñ Check QUICKSTART.md for step-by-step usage instructions${NC}"
    echo ""
}

# Function to show logs for problematic services
show_logs() {
    echo -e "${BLUE}üìã Recent logs for key services:${NC}"
    echo ""
    
    for service in lakehouse-init airflow-init airflow-scheduler airflow-webserver superset jupyter spark-master; do
        echo -e "${YELLOW}=== $service ===${NC}"
        docker compose logs --tail=20 "$service" 2>/dev/null || echo "Service not running or no logs available"
        echo ""
    done
}

# Function to check system resources
check_resources() {
    echo -e "${BLUE}üíª System Resource Check${NC}"
    echo -e "${BLUE}========================${NC}"
    echo ""
    
    # Check available memory
    if command -v free &> /dev/null; then
        echo -e "${YELLOW}Memory Usage:${NC}"
        free -h
        echo ""
    fi
    
    # Check disk space
    if command -v df &> /dev/null; then
        echo -e "${YELLOW}Disk Usage:${NC}"
        df -h . 2>/dev/null || echo "Cannot check disk usage"
        echo ""
    fi
    
    # Check Docker system
    echo -e "${YELLOW}Docker System Info:${NC}"
    docker system df 2>/dev/null || echo "Cannot get Docker system info"
    echo ""
    
    # Recommend configuration
    echo -e "${YELLOW}üí° Configuration Recommendations:${NC}"
    echo -e "  ‚Ä¢ Standard setup: 16GB RAM, 4+ CPU cores"
    echo -e "  ‚Ä¢ Fat server setup: 64GB+ RAM, 16+ CPU cores"
    echo -e "  ‚Ä¢ Copy .env.fat-server to .env for high-performance systems"
    echo ""
}

# Function to clean up and reset
reset_environment() {
    echo -e "${RED}üßπ Resetting Lakehouse Lab environment...${NC}"
    echo -e "${RED}This will destroy ALL data and containers!${NC}"
    echo ""
    
    read -p "Are you absolutely sure? Type 'yes' to continue: " -r
    echo
    if [[ $REPLY == "yes" ]]; then
        echo -e "${YELLOW}Stopping all services...${NC}"
        docker compose down -v
        
        echo -e "${YELLOW}Removing data directory...${NC}"
        rm -rf "$LAKEHOUSE_ROOT"
        
        echo -e "${YELLOW}Pruning Docker system...${NC}"
        docker system prune -f
        
        echo -e "${GREEN}‚úÖ Environment reset complete${NC}"
        echo -e "${BLUE}üí° Run './start-lakehouse.sh' to start fresh${NC}"
    else
        echo -e "${YELLOW}Reset cancelled${NC}"
    fi
}

# Function to show status
show_status() {
    echo -e "${BLUE}üìä Lakehouse Lab Status${NC}"
    echo -e "${BLUE}=======================${NC}"
    echo ""
    
    echo -e "${YELLOW}Container Status:${NC}"
    docker compose ps
    echo ""
    
    echo -e "${YELLOW}Service Health:${NC}"
    services=(
        "MinIO:http://localhost:9000/minio/health/live"
        "Spark:http://localhost:8080"
        "Airflow:http://localhost:9020/health"
        "Superset:http://localhost:9030/health"
        "Portainer:http://localhost:9060"
    )
    
    for service_info in "${services[@]}"; do
        IFS=':' read -r name url <<< "$service_info"
        if curl -sf "$url" >/dev/null 2>&1; then
            echo -e "  ‚úÖ $name is healthy"
        else
            echo -e "  ‚ùå $name is not responding"
        fi
    done
    echo ""
}

# Main script logic
case "$STARTUP_MODE" in
    "logs")
        show_logs
        ;;
    "reset")
        reset_environment
        ;;
    "stop")
        echo -e "${YELLOW}üõë Stopping Lakehouse Lab...${NC}"
        docker compose down
        echo -e "${GREEN}‚úÖ All services stopped${NC}"
        ;;
    "status")
        show_status
        ;;
    "resources")
        check_resources
        ;;
    "help"|"-h"|"--help")
        echo -e "${BLUE}Lakehouse Lab Startup Script${NC}"
        echo ""
        echo -e "${YELLOW}Usage:${NC}"
        echo -e "  ./start-lakehouse.sh [mode]"
        echo ""
        echo -e "${YELLOW}Modes:${NC}"
        echo -e "  normal     Start all services (default)"
        echo -e "  debug      Start services layer by layer"
        echo -e "  minimal    Start only core services"
        echo -e "  stop       Stop all services"
        echo -e "  status     Show service status"
        echo -e "  logs       Show recent logs"
        echo -e "  resources  Check system resources"
        echo -e "  reset      Reset entire environment (DESTRUCTIVE)"
        echo -e "  help       Show this help"
        echo ""
        ;;
    *)
        start_with_dependencies
        ;;
esac
