name: Backup System Test

on:
  push:
    branches: [ main, dev ]
    paths:
      - 'scripts/backup-lakehouse.sh'
      - 'scripts/restore-lakehouse.sh'
      - 'examples/cron-backup-setup.sh'
      - 'templates/airflow/dags/lakehouse_backup_dag.py'
      - 'docker-compose*.yml'
  pull_request:
    branches: [ main, dev ]
    paths:
      - 'scripts/backup-lakehouse.sh'
      - 'scripts/restore-lakehouse.sh' 
      - 'examples/cron-backup-setup.sh'
      - 'templates/airflow/dags/lakehouse_backup_dag.py'
      - 'docker-compose*.yml'

jobs:
  backup-system-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Generate test credentials
      run: |
        # Create .env for testing
        if [ -f scripts/generate-credentials.sh ]; then
          ./scripts/generate-credentials.sh
        else
          cp .env.default .env
        fi
        
        echo "COMPOSE_PROJECT_NAME=lakehouse-backup-test" >> .env
        echo "HOST_IP=localhost" >> .env

    - name: Verify backup script structure
      run: |
        echo "üîç Verifying backup system components..."
        
        # Check that backup scripts exist and are executable
        [ -f scripts/backup-lakehouse.sh ] && [ -x scripts/backup-lakehouse.sh ]
        [ -f scripts/restore-lakehouse.sh ] && [ -x scripts/restore-lakehouse.sh ]
        [ -f examples/cron-backup-setup.sh ] && [ -x examples/cron-backup-setup.sh ]
        
        echo "‚úÖ All backup scripts present and executable"
        
        # Check Airflow DAG template
        [ -f templates/airflow/dags/lakehouse_backup_dag.py ]
        
        # Validate Python syntax of DAG
        python -m py_compile templates/airflow/dags/lakehouse_backup_dag.py
        echo "‚úÖ Airflow DAG template syntax valid"

    - name: Test backup script help and options
      run: |
        echo "üìù Testing backup script help and options..."
        
        # Test help output
        ./scripts/backup-lakehouse.sh --help | grep -q "Usage:"
        ./scripts/restore-lakehouse.sh --help | grep -q "Usage:"
        
        # Test dry-run mode
        ./scripts/backup-lakehouse.sh --dry-run --services postgres
        ./scripts/backup-lakehouse.sh --dry-run --exclude-services portainer
        ./scripts/backup-lakehouse.sh --dry-run --compress --verify
        
        echo "‚úÖ Backup script options working"

    - name: Test CRON setup script
      run: |
        echo "‚è∞ Testing CRON setup script..."
        
        # Test CRON setup dry-run
        ./examples/cron-backup-setup.sh --dry-run
        ./examples/cron-backup-setup.sh --dry-run --schedule "0 3 * * 0" --compress
        
        echo "‚úÖ CRON setup script working"

    - name: Start minimal services for backup testing
      run: |
        echo "üöÄ Starting minimal services for backup testing..."
        
        # Start only core services needed for backup testing
        docker compose up -d postgres minio lakehouse-init
        sleep 30
        
        # Verify services are running
        docker compose ps
        
        # Wait for services to be ready
        timeout 60 bash -c 'until curl -f http://localhost:9000/minio/health/live; do sleep 2; done'
        timeout 60 bash -c 'until docker exec lakehouse-backup-test-postgres-1 pg_isready -U postgres; do sleep 2; done'

    - name: Create test data
      run: |
        echo "üìù Creating test data for backup verification..."
        
        # Create PostgreSQL test data
        docker exec lakehouse-backup-test-postgres-1 psql -U postgres -d lakehouse -c "
          CREATE TABLE IF NOT EXISTS backup_test (
            id SERIAL PRIMARY KEY,
            test_data VARCHAR(255),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          );
          INSERT INTO backup_test (test_data) VALUES ('backup-test-data-$(date +%s)');
        "
        
        # Create MinIO test data
        echo "test-backup-data-$(date +%s)" > test-backup-file.txt
        
        docker run --rm \
          --network lakehouse-backup-test_lakehouse \
          -v $(pwd)/test-backup-file.txt:/tmp/test-file.txt \
          minio/mc:latest \
          bash -c "
            mc alias set testminio http://minio:9000 \${MINIO_ROOT_USER:-admin} \${MINIO_ROOT_PASSWORD} &&
            mc cp /tmp/test-file.txt testminio/lakehouse/backup-test.txt
          "
        
        echo "‚úÖ Test data created"

    - name: Test backup creation (dry-run)
      run: |
        echo "üíæ Testing backup creation (dry-run)..."
        
        # Test various backup configurations
        ./scripts/backup-lakehouse.sh --dry-run --services postgres,minio
        ./scripts/backup-lakehouse.sh --dry-run --compress --verify
        ./scripts/backup-lakehouse.sh --dry-run --parallel
        ./scripts/backup-lakehouse.sh --dry-run --retention-days 7
        
        echo "‚úÖ Backup dry-run tests passed"

    - name: Test actual backup creation
      run: |
        echo "üíæ Testing actual backup creation..."
        
        # Create a real backup (small services only for testing)
        ./scripts/backup-lakehouse.sh --services postgres --output-dir /tmp/test-backup --quiet
        
        # Verify backup was created
        [ -d /tmp/test-backup ]
        
        # Find the backup directory
        BACKUP_DIR=$(find /tmp/test-backup -maxdepth 1 -type d -name "lakehouse-backup-*" | head -1)
        [ -n "$BACKUP_DIR" ]
        
        echo "Backup created at: $BACKUP_DIR"
        ls -la "$BACKUP_DIR"
        
        # Verify backup contains expected components
        [ -f "$BACKUP_DIR/backup-metadata.json" ]
        
        echo "‚úÖ Backup creation successful"

    - name: Test restore script (dry-run)
      run: |
        echo "üîÑ Testing restore script..."
        
        # Find the backup ID
        BACKUP_ID=$(find /tmp/test-backup -maxdepth 1 -type d -name "lakehouse-backup-*" -exec basename {} \; | head -1)
        
        if [ -n "$BACKUP_ID" ]; then
          # Test restore dry-run
          ./scripts/restore-lakehouse.sh "$BACKUP_ID" --backup-dir /tmp/test-backup --dry-run
          ./scripts/restore-lakehouse.sh "$BACKUP_ID" --backup-dir /tmp/test-backup --service postgres --dry-run
          
          echo "‚úÖ Restore dry-run tests passed"
        else
          echo "‚ö†Ô∏è No backup found for restore testing"
        fi

    - name: Test backup metadata and verification
      run: |
        echo "üîç Testing backup metadata and verification..."
        
        BACKUP_DIR=$(find /tmp/test-backup -maxdepth 1 -type d -name "lakehouse-backup-*" | head -1)
        
        if [ -n "$BACKUP_DIR" ] && [ -f "$BACKUP_DIR/backup-metadata.json" ]; then
          # Validate metadata structure
          python -c "
import json
with open('$BACKUP_DIR/backup-metadata.json', 'r') as f:
    metadata = json.load(f)
    assert 'backup_id' in metadata
    assert 'timestamp' in metadata
    assert 'services' in metadata
    print('‚úÖ Backup metadata structure valid')
"
        else
          echo "‚ö†Ô∏è Backup metadata not found for verification"
        fi

    - name: Generate backup system report
      run: |
        echo "üìä Backup System Test Report"
        echo "============================"
        echo "‚úÖ All backup system tests passed!"
        echo ""
        echo "Verified components:"
        echo "  ‚Ä¢ Backup script execution and options"
        echo "  ‚Ä¢ Restore script functionality"
        echo "  ‚Ä¢ CRON setup script"
        echo "  ‚Ä¢ Airflow DAG template"
        echo "  ‚Ä¢ Backup metadata generation"
        echo "  ‚Ä¢ Dry-run modes for safety"
        echo ""
        echo "Test backup created:"
        find /tmp/test-backup -name "lakehouse-backup-*" -type d | head -1 | xargs ls -la

    - name: Cleanup test environment
      if: always()
      run: |
        echo "üßπ Cleaning up test environment..."
        docker compose down -v --remove-orphans || true
        rm -rf /tmp/test-backup || true
        rm -f test-backup-file.txt || true
        docker system prune -f || true